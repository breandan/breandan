%! Author = breandan
%! Date = 2/22/21

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}

% Document

\title{Learning to navigate, read and apply\\software documentation like a human}
\author{Breandan Considine, Xujie Si, Jin Guo}
\begin{document}
\maketitle

\section{Introduction}

Humans are adept information foraging agents. We can quickly find relevant information in a large corpus by recognizing and following textual landmarks. Software projects feature a variety of semi-structured documents containing many such clues indicating where contextually relevant information can be found. In this work, we train an agent to solve a programming task, by learning to navigate and read software artifacts like source code and documentation in order to facilitate a goal-directed programming task.

Our work broadly falls under the umbrella of text-based reinforcement learning. Prior literature falls under two categories: natural or formal language. Reinforcement learning (RL) in the natural domain typically focuses on question answering~\cite{buck2017ask, chen2019reinforcement}, or interactive text games~\cite{he2015deep,ammanabrolu2018playing,narasimhan2015language,guo2020interactive,ammanabrolu2020graph}. RL techniques have begun to show promising results for program synthesis~\cite{ellis2019write, johnson2020learning, chen2020program}. Our work falls at the intersection of these two domains.

Early work in program learning realized the importance of graph-based representations~\cite{allamanis2017learning}, however explicit graph construction requires extensive feature-engineering. More recent work in program synthesis has explored incorporating a terminal~\cite{ellis2019write}, graphical~\cite{walke2020learning} or other user interface to explore the space of valid programs. Adaption to settings where style, terminology and document structure vary significantly, remains a challenge. Prior approaches do not consider the scope or variety of artifacts in a software project. Some attempt to learn a graph~\cite{johnson2020learning} but only consider the local structure of source code and still require an explicit parser of some kind.

Unlike prior work, we consider the whole project and related artifacts. Instead of parsing their contents explicitly, which may be computationally expensive or too large to fit in memory, we allow the agent to construct the graph organically by exploring the filesystem, which can later be decoded to predict a label or sequence at inference time.

Consider a newly-hired developer, who has programming experience, but no prior knowledge about a closed-source project. After onboarding, she gets access to the project and is assigned her first ticket: Fix test broken by \texttt{0fb98be}. After finding the commit and getting familiar the codebase, she searches on StackOverflow, finds a relevant solution, then copies some code into the editor, makes a few changes, presses run, and the test passes.

In order to accomplish some task, such as fixing a test or finding a bug, an information-seeking agent must locate resources in a database to solve the task. Similar to a human programmer, it needs to perform simple actions, like \texttt{search(query)}, \texttt{read(text)}, \texttt{copy(text, from, to)}, and \texttt{runCode()}. During each rollout, the agent can explore the environment using the primitives available to build a lazily instantiated knowledge graph.

As the agent is exploring the environment, it collects information about how to solve the task. Similar to navigating a physical environment, it can visit relevant documents and source code artifacts. As the agent further explores the project, it gathers information in a latent state, which can be decoded at inference time to obtain the predicted solution.

During evaluation, we then measure how well the agent performed. Many loss functions are possible depending on the task, from a simple distance metric on a deleted fragment of code, to a more complex property (e.g. presence or absence of an error, or some property of the output) which must be satisfied by interacting with the runtime environment. Many program analysis and repair tasks are amenable to the described setup, including defect, duplicate or vulnerability detection and correction.

\section{Method}

MDP / GNNs / Transformers / REINFORCE

\section{Experiments}

In this work, our primary focus is learning to read software artifacts in a software repository and apply the knowledge contained. Code is a graph database, or knowledge graph whose edges represent semantic relationships between entities. Our research seeks to answer the following research questions. Can the agent can learn to:

\begin{enumerate}
  \item Navigate the project and locate contextually relevant artifacts?
  \item Comprehend the semantic content of contextually relevant artifacts?
  \item Apply the knowledge gathered to perform the assigned task?
\end{enumerate}

In our first experiment, we attempt to understand which queries the agent is performing when solving a programming task. Does it search for certain keywords in the context?

In our second experiment, we try to measure the information contained in various filetypes by ablating results in various filetypes. For trajectories which explore various filetypes such as Markdown or Java, what information gain do resources of these filetypes provide?

Our next steps are to build an RL environment which allows an agent to interact with a knowledge base and runtime environment. We will use an in-memory graph database to store and load project artifacts to and from the knowledge base. To model the agent, we will use the actions described above for navigating the environment, searching for text and reading documents, with a transformer-based internal state, pretrained on a corpus of projects in the same language. Finally, to evaluate the agent, we will generate problem instances and use a JIT to rapidly validate generated code fragments.

  \bibliography{research_proposal}
  \bibliographystyle{plain}
\end{document}
%! Author = breandanconsidine
%! Date = 8/9/21

% Preamble
\documentclass[10pt]{article}

% Packages
\usepackage{amsmath}

\usepackage{hyperref}
\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}

% Document
\title{Programming in the Age of Intelligent Machines}
\author{Breandan Considine}
\date{\today}

\begin{document}
  \maketitle
  \section{Introduction}

Since the invention of modern computers in the mid 20th century, computer programming has undergone a number of paradigm shifts. From the rise of functional programming, to dynamic, object-oriented, and type-level programming, to the availability of myriad tools and frameworks -- its practitioners have witnessed a veritable Renaissance in the art of computer programming. With each of these paradigm shifts, programmers have realized new conceptual frameworks for reasoning and expressing their ideas more clearly and concisely.

Over the last few years, another paradigm shift has been set in motion, with significant implications for how we think about and write programs in the coming century. By most measures, computers have grown steadily more intelligent and capable of assisting programmers with mentally taxing chores. For example, intelligent programming tools (IPTs) powered by neural language models have this year helped over 10 million unique human beings program computers. As IPTs help digitally illiterate communities to discover their innate aptitude for computer programming, this population will continue to rise.

Computer programming is a uniquely creative exercise among the range of human activities. It channels our innate linguistic, logical, imaginative, and social abilities to bring abstract ideas into reality, and ultimately, gives humans the freedom to create new realities of their own design. In collaboration with other humans and the increasing participation of IPTs, vast and elaborate virtual worlds have been manufactured, where the majority of humankind now chooses to spend their daily lives. With the expanding opportunities these new digital frontiers promise, their population too will continue to grow.

Today IPTs share an equal role in shaping many aspects of computer programming, from knowledge discovery to API design, and program synthesis to validation and verification. However, this balance is shifting beneath our feet. Once its creators, programmers are now primarily consumers of information provided by an IPT, and increasingly rely on them to perform their daily work. With the unique opportunities and risks this partnership presents, what division of labor should exist between humans and our new coding collaborators? This is the question we have set out to understand in the following literature review.

  \section{Neural Language Models for Source Code}

Programming researchers have long held an interest in using intelligent tools to help them write programs~\cite{bras1993artificial}. Due to fundamental limitations in data and processing power, many of these ambitions had not come to pass until the last few years, thanks to the availability of \textit{big code}~\cite{allamanis2018survey}, the development of differentiable programming libraries for gradient-based learning~\cite{baydin2018automatic}, and attention-based language models~\cite{vaswani2017attention}, among other technical achievements. Armed with this new repertoire, programming researchers have revisited their interest in IPTs. Naturally, one of the first applications considered was code completion.

 Following their initial success in natural language, rapid progress continues to be made in the application and specialization of transformers to source code, as well as industrial transfer where this technology is now trained and deployed on millions of programmers worldwide~\cite{chen2021evaluating}. Given a natural language description of an incomplete method, these models are capable of inferring programmer intent and completing multiline code snippets. Progress is expected to improve.

The problem comes down to a question of grammar induction. Based on empirical results, fixed-precision transformers (e.g. GPT-2, BERT) are thought capable of recognizing the class of counter languages~\cite{bhattamishra2020ability}, i.e. somewhere between context-free and context-sensitive, although this characterization requires a more careful theoretical analysis. For source code typically stored on GitHub, this class would appear to suffice -- models trained on such datasets are currently capable of sketching rudimentary programs and boilerplate code templates, however more complex fragments require additional oversight.

An important shortcoming of imitation learning is the question of data provenance and validation: even if the training data is syntactically well-formed, constraints on the class of valid programs are ill-posed. As a consequence, a large fragment of languages generated may be syntactically valid but semantically unsound, i.e. may throw undesired runtime errors, or appear to work at first, but are in fact broken in a subtle manner. Like most language models of its kind, performance is highly sensitive to the dataset quality, as common errors in the training data are prone to be inherited and reproduced by an IPT.

The vast majority of modern programming consists of writing ceremonial boilerplate, tasks for which neural language models are well-suited. A tremendous amount of human labor is spent on such chores, and reallocating those resources towards more intellectually stimulating tasks may encourage a larger demographic to become programmers who would otherwise lack the patience or interest. By removing these barriers to entry, programmers can more quickly arrive at the rewarding parts of program design and implementation.

Nevertheless, mere imitation is a somewhat dissatisfying approximation to programming from a computer-scientific perspective -- lacking in some essential aspect the qualities its practitioners aspire to fulfill. Helpful though it may be for tedious chores, the art of programming is not reading gigabytes of code and minimizing a cross-entropy loss. Programming requires imagination, creativity, problem-solving -- qualities which cannot be conferred by simply scaling existing language models with more data and parameters. What could be missing?

\section{Knowledge Discovery and Neural Code Search}

For many decades, computer scientists have pondered the nature of search. Search is an indispensable tool in the working programmer's repertoire and goes to the heart of many fundamental problems in artificial intelligence, from classical to statistical optimization, and information retrieval to computational linguistics. Programming itself can be seen as a kind of search-based optimization problem~\cite{alur2018search}, consistent with its original mathematical interpretation, e.g. linear or stochastic programming. Coupled with a grammatical template, one could imagine searching through the space of valid programs to produce a higher-order function satisfying some criteria. Indeed, this exact setup is routinely-studied in the annual syntax-guided synthesis (SyGuS) competition~\cite{alur2016sygus}.

Returning to our earlier question of, ``What else could human programmers be doing besides imitation learning?'', one plausible answer could be trial and error. Given some program specification and a computational budget, a na√Øve strategy could be to simply evaluate as many programs from a dataset of candidate solutions as possible within the allotted budget. Many programmers do in fact practice this style of copy-paste programming as evidenced by duplicate code studies~\cite{lopes2017dejavu}, a problem known to adversely bias machine learning models, and which must be corrected for during data curation~\cite{allamanis2019adverse}.

For nearly all practical programming languages, the space of valid programs can hardly be enumerated, never mind evaluated in a reasonable amount of time. A more refined strategy is needed: for example, we could select a small set of reusable building blocks, then compose and evaluate partial programs using an execution-guided scheme~\cite{chen2018execution, wang2018execution}. By interacting with an interpreter, we may be able to arrive at a solution via incremental improvement. As with most dynamic programming algorithms, the problem comes down to a question of substructure: if modification invalidates prior work, search can become exponential or worse.

For example, many useful programs belong to the class of context free languages. Sampling is possible using a probabilistic grammar, but at what cost? The number of distinct parse trees grows super-exponentially with height. Various strategies have been designed to inhibit this growth, but even with judicious pruning, the topology of many languages does not admit an efficient beam search, or the cost required may be prohibitive. Yet humans are able to solve many computationally hard search problems with deceptive ease. How?

One possibility is that humans possess more computational resources than might we give them credit for, and a similarly-enriched neural-guided search would be equally capable. Another hypothesis is that we are not \textit{searching} for programs as such, or at least thinking about this process in the wrong terms. When someone is painting a portrait or writing a novel, we do not call this search. Likewise, programming is not always about searching for an answer, but finding the right question to ask, of understanding, exploring and visualizing the design space, for which there is no specification. The program acquires a meaning of its own as the medium for a dialog between human and machine. In the following sections, we explore two contrasting models for this dialog, one where the human is the teacher~\ref{sec:automatic-and-declarative-programming}, and one where the IPT is the teacher~\ref{sec:computer-aided-reasoning-tools}.

  \section{Automatic and Declarative Programming}\label{sec:automatic-and-declarative-programming}

In The Art of Computer Programming~\cite{knuth1997art}, Donald Knuth memorably writes, ``Programs are meant to be read by humans and only incidentally for computers to execute.'' Taking this perspective, one may be tempted to ask, ``Why must programming languages be so difficult that we need IPTs to write down our ideas in the first place?'' If we consider programming chiefly a matter of communicating human intent to machines, language designers should make every effort to simplify the language so that users may express their intent in a more natural way, then harness machine intelligence in the service of fulfilling those goals. Known as automatic programming, this approach has been well-studied in languages like SQL, Prolog and miniKanren.

The idea of what is today known as declarative programming began as early as the 1950s, when researchers applied the tools of mathematical logic to study optimization. In this early work, programmers would first state their problem in a mathematical language, such as a linear or quadratic program with a set of constraints. The Soviets used it to study many questions related to optimal transport and control. It also turns out to be very useful for other things.

The same tools can be applied to programming in a more general manner. What other kinds of things can automatic programming languages generate? Not only can automatic programming produce numerical solutions. By considering the same techniques over different algebras, we can generate differently-shaped objects, e.g. graphs, trees and so forth. These things are called algebraic data types. For example, we could use ADTs to represent a higher-order functions, i.e. for program synthesis if we so desired.

Our primary interest in automatic programming is twofold: (1) it is basis of the first successful approach to wide-scale gradient-based learning~\cite{baydin2018automatic}, and (2) as a practical framework for realizing the once-ridiculed~\cite{dijkstra1979foolishness} idea of natural language programming, now increasingly plausible through powerful language models.

The problem with automatic programming is that the only means of feedback to the user is by outputting a solution, or failing that, producing lots of heat. It would be nice to have a more interactive mechanism.

  \section{Computer-Aided Reasoning Tools}\label{sec:computer-aided-reasoning-tools}

  The programmer is not the teacher, the programmer is actually the student.

When programmers ask for an intelligent programming tool, what they really want is not a subservient genie who grants wishes without question, but a teacher who rapidly gives them feedback about the implications of their design choices within the confines of a well-defined language. This is one advantage of a type system: not only does it provide highly precise feedback, but also allows us to sample the space of constrained programs which is semantically correct. Thus the interaction is bidirectional.

  \section{Future Directions of Computer Programming}

  \section{Conclusion}

  \bibliography{literature_review}
  \bibliographystyle{plain}
\end{document}
%! Author = breandanconsidine
%! Date = 8/9/21

% Preamble
\documentclass[10pt]{article}

% Packages
\usepackage{amsmath}

% Document
\title{Programming in the Age of Intelligent Machines}
\author{Breandan Considine}
\date{\today}

\begin{document}
  \maketitle
  \section{Introduction}

Since the invention of modern computers in the mid 20th century, computer programming has undergone a number of paradigm shifts. From the rise of functional programming, to dynamic, object-oriented, and type-level programming, to the availability of myriad tools and frameworks -- its practitioners have witnessed a veritable Renaissance in the art of computer programming. With each of these paradigm shifts, programmers have realized new conceptual frameworks for reasoning and expressing their ideas more clearly and concisely.

Over the last few years, another paradigm shift has been set in motion, with significant implications for how we think about and write programs in the coming century. By most measures, computers have grown steadily more intelligent and capable of assisting programmers with mentally taxing chores. For example, intelligent programming tools (IPTs) powered by neural language models have this year helped over 10 million unique human beings program computers. As IPTs help digitally illiterate communities to discover their innate aptitude for computer programming, this population will continue to rise.

Computer programming is a uniquely creative exercise among the range of human activities. It channels our innate linguistic, logical, imaginative, and social abilities to bring abstract ideas into reality, and ultimately, gives humans the freedom to create new realities of their own design. In collaboration with other humans and the increasing participation of IPTs, vast and elaborate virtual worlds have been manufactured, where the majority of humankind now chooses to spend their daily lives. With the expanding opportunities these new digital frontiers promise, their population too will continue to grow.

Today IPTs share an equal role in shaping many aspects of computer programming, from knowledge discovery to API design, and program synthesis to validation and verification. However, this balance is shifting beneath our feet. Once its creators, programmers are now primarily consumers of information provided by an IPT, and increasingly rely on them to perform their daily work. With the unique opportunities and risks this partnership presents, what division of labor should exist between humans and our new coding collaborators? This is the question we have set out to understand in the following literature review.

  \section{Neural Language Models for Source Code}

Programming researchers have long held an interest in using intelligent tools to help them write programs~\cite{bras1993artificial}. Due to fundamental limitations in data and processing power, many of these ambitions did not come to fruition until the last few years, thanks the availability of \textit{big code}~\cite{allamanis2018survey}, the development of differentiable programming libraries for gradient-based learning~\cite{baydin2018automatic}, and attention-based language models~\cite{vaswani2017attention}, among other technical achievements. Armed with this new repertoire, programming researchers have revisited their interest in IPTs. Naturally, one of the first applications considered was code completion.

 Following their initial success in natural language, rapid progress continues to be made in the application and specialization of transformers to source code, as well as industrial transfer where this technology is now trained and deployed on millions of programmers worldwide~\cite{chen2021evaluating}. Given a natural language description of an incomplete method, these models are capable of inferring programmer intent and completing multiline code snippets. Progress is expected to improve.

The problem comes down to a question of grammar induction. Based on empirical results, fixed-precision transformers (e.g. GPT-2, BERT) are thought capable of recognizing the class of counter languages~\cite{bhattamishra2020ability}, i.e. somewhere between context-free and context-sensitive, although this characterization requires a more careful theoretical analysis. For source code typically stored on GitHub, this class would appear to suffice -- models trained on such datasets are currently capable of rudimentary program sketching and boilerplate code completion, however more complex fragments require additional oversight.

An important shortcoming of imitation learning is the question of data provenance and validation. Even if the training data is syntactically and semantically valid, constraints on the class of valid programs are ill-posed. As a consequence, a large fragment of programs generated may be syntactically valid but semantically unsound, i.e. may throw runtime errors at best, or appear to work at first, but are in fact broken in a subtle manner. Like most language models of its kind, performance is highly sensitive to the dataset quality, as common errors in the training data can be inherited and reproduced.

The vast majority of modern programming consists of writing ceremonial boilerplate, tasks for which neural language models are well-suited. A tremendous amount of human labor is spent on such chores, and reallocating those resources towards more intellectually stimulating tasks may encourage a larger demographic to become programmers who would otherwise lack the patience or interest. By removing these barriers to entry, programmers can more quickly arrive at the rewarding parts of program design and implementation.

Nevertheless, mere imitation is a somewhat dissatisfying approximation to programming from a computer-scientific perspective -- lacking in some essential aspect the qualities its practitioners aspire to fulfill. Helpful though it may be for tedious chores, the art of programming is not reading gigabytes of code and minimizing a cross-entropy loss. Programming requires imagination, creativity, problem-solving -- qualities which cannot be conferred by simply scaling existing language models with more data and parameters. What could be missing?

\section{Knowledge Discovery and Neural Code Search}

For decades, computer scientists have pondered the nature of search. Search is an indispensable tool in the programmer's repertoire and goes to the heart of many problems in computer science and software engineering. Search is also widely studied in artificial intelligence, from classical to statistical optimization, and information retrieval to computational linguistics. Programming itself can be seen as a kind of search-based optimization problem~\cite{alur2018search}, similar to its etymological use in e.g. linear or stochastic programming. Coupled with a formal language and possibly a set of examples, one could imagine searching through the space of valid programs to produce a function which satisfies some desired criteria. A similar task is considered in the SyGuS competition.~\cite{alur2016sygus}.

Returning to our earlier question of, ``What else could human programmers be doing besides imitation learning?'', one plausible answer could be trial and error. Given some program specification and a computational budget, a na√Øve strategy could be to simply enumerate all valid programs, and evaluate as many as possible within the computational budget allotted. Given a dataset of previous solutions, we could prioritize the reuse of similar code snippets. Many programmers do in fact practice this style of copy-paste programming as evidenced by duplicate code studies~\cite{lopes2017dejavu}, a problem known to bias to machine learning models and which must be corrected for during data curation~\cite{allamanis2019adverse}.

For nearly all practical programming languages, the space of valid programs can hardly be enumerated, never mind evaluated in a reasonable amount of time. A more refined strategy is needed: for example, we could select a small set of reusable building blocks, then compose and evaluate partial programs using an execution-guided scheme~\cite{chen2018execution, wang2018execution}. By interacting with an interpreter, it may be possible to arrive at a solution by incrementally constructing subprograms. As with many dynamic programming algorithms of this kind, the problem comes down to a question of overlapping subproblems and optimal substructure: if modification invalidates prior work, search can become exponential or worse.

%For example, many useful programs belong to the class of context free languages. Sampling this space is possible using a PCFG, but requires exponential time...

Depending on the optimization problem, other strategies have been designed such as backtracking and random restarts to mitigate the effect of rabbit holes and dead ends. Even with judicious pruning, the topology of many program synthesis problems simply does not admit an efficient beam search procedure, or the cost required to execute it may be prohibitive. Yet humans are able to solve many computationally hard search problems with deceptive ease. How?

One possibility is that humans possess more computational resources than might we give them credit for, and a similarly-enriched neural-guided search would be equally capable. Another hypothesis is that we are not \textit{searching} for programs per se, or at least using the wrong language to describe this process. When someone is painting a portrait or writing a novel, we do not call this search. Likewise, programming is not always about searching for an answer, but finding the right question to ask, of visualizing, understanding, and exploring the unknown. There is no specification because the program creates its own evolving meaning over time, is an expression of pure unbridled creativity.

  \section{Automatic and Synthetic Programming}

Another idea is to invert the interaction model and the question of ``source code'' altogether. Why are we synthesizing source code if it's just a means of telling computers what to do? The code \textit{is} the specification. We're the ones who are supposed to specifying the behavior, so we should be writing the code! Let the computer figure out how to run the low-level details, but sketch the high level idea. This is the idea behind automatic programming.

  \section{Computer-Aided Reasoning Tools}

When programmers ask for an intelligent programming tool, what they really want is not a subservient genie who grants wishes without question, but a teacher who rapidly gives them feedback about the implications of their design choices within the confines of a well-defined language. This is one advantage of a type system: while not itself particularly intelligent by any measure, it is at least reliable.

  \section{Future Directions of Computer Programming}

  \section{Conclusion}

  \bibliography{literature_review}
  \bibliographystyle{plain}
\end{document}
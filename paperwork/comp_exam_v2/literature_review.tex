%! Author = breandanconsidine
%! Date = 8/9/21

% Preamble
\documentclass[10pt]{article}

% Packages
\usepackage{amsmath}
\usepackage[pdf]{graphviz}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{tikz}
\usepackage{listings}

\newcommand*\circled[1]{\tikz[baseline=-0.1cm]{\node[shape=circle,draw,inner sep=0.48pt] (char) {\fontsize{7}{12}\textsf{#1}};}}

\usepackage{unicode-math}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\usepackage{cancel}
\newcommand{\nDownarrow}{\ensuremath{\text{ }\cancel{\Downarrow}\text{ }}}
\usepackage{centernot}

\usepackage{bussproofs}
\usepackage{tabularx}

\usepackage{hyperref}
\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}

% Document
\title{Programming in the Age of Intelligent Machines}
\author{Breandan Considine}
\date{\today}

\begin{document}
  \maketitle
  \section{Introduction}

Since the invention of modern computers in the mid 20th century, computer programming has undergone a number of paradigm shifts. From the rise of functional programming, to dynamic, object-oriented, and type-level programming, to the availability of myriad tools and frameworks -- its practitioners have witnessed a veritable Renaissance in the art of computer programming. With each of these paradigm shifts, programmers have realized new conceptual frameworks for reasoning and expressing their ideas more clearly and concisely.

Over the last few years, another paradigm shift has been set in motion, with significant implications for how we think about and write programs in the coming century. By most measures, computers have grown steadily more intelligent and capable of assisting programmers with mentally taxing chores. For example, intelligent programming tools (IPTs) powered by neural language models have this year helped over 10 million unique human beings program computers. As IPTs help digitally illiterate communities to discover their innate aptitude for computer programming, this population will continue to rise.

Computer programming is a uniquely creative exercise among the range of human activities. It channels our innate linguistic, logical, imaginative, and social abilities to bring abstract ideas into reality, and ultimately, gives humans the freedom to create new realities of their own design. In collaboration with other humans and the increasing participation of IPTs, vast and elaborate virtual worlds have been manufactured, where the majority of humankind now chooses to spend their daily lives. With the expanding opportunities these new digital frontiers promise, their population too will continue to grow.

Today IPTs share an equal role in shaping many aspects of computer programming, from knowledge discovery to API design, and program synthesis to validation and verification. However, this balance is shifting beneath our feet. Once its creators, programmers are now primarily consumers of information provided by an IPT, and increasingly rely on them to perform their daily work. With the unique opportunities and risks this partnership presents, what division of labor should exist between humans and our new coding collaborators? This is the question we have set out to understand in the following literature review.

  \section{Neural Language Models for Source Code}

Programming researchers have long held an interest in using intelligent tools to help them write programs~\cite{bras1993artificial}. Due to fundamental limitations in data and processing power, many of these ambitions had not come to pass until the last few years, thanks to the availability of \textit{big code}~\cite{allamanis2018survey}, the development of differentiable programming libraries for gradient-based learning~\cite{baydin2018automatic}, and attention-based language models~\cite{vaswani2017attention}, among other technical achievements. Armed with this new repertoire, programming researchers have revisited their interest in IPTs. Naturally, one of the first applications considered was code completion.

 Following their initial success in natural language, rapid progress continues to be made in the application and specialization of transformers to source code, as well as industrial transfer where this technology is now trained and deployed on millions of programmers worldwide~\cite{chen2021evaluating}. Given a natural language description of an incomplete method, these models are capable of inferring programmer intent and completing multiline code snippets. Progress is expected to improve.

The problem comes down to a question of grammar induction. Based on empirical results, fixed-precision transformers (e.g. GPT-2, BERT) are thought capable of recognizing the class of counter languages~\cite{bhattamishra2020ability}, i.e. somewhere between context-free and context-sensitive, although this characterization requires a more careful theoretical analysis. For source code typically stored on GitHub, this class would appear to suffice -- models trained on such datasets are currently capable of sketching rudimentary programs and boilerplate code templates, however more complex fragments require additional oversight.

An important shortcoming of imitation learning is the question of data provenance and validation: even if the training data is syntactically well-formed, constraints on the class of valid programs are ill-posed. As a consequence, a large fragment of languages generated may be syntactically valid but semantically unsound, i.e. may throw undesired runtime errors, or appear to work at first, but are in fact broken in a subtle manner. Like most language models of its kind, performance is highly sensitive to the dataset quality, as common errors in the training data are prone to be inherited and reproduced by an IPT.

The vast majority of modern programming consists of writing ceremonial boilerplate, tasks for which neural language models are well-suited. A tremendous amount of human labor is spent on such chores, and reallocating those resources towards more intellectually stimulating tasks may encourage a larger demographic to become programmers who would otherwise lack the patience or interest. By removing these barriers to entry, programmers can more quickly arrive at the rewarding parts of program design and implementation.

Nevertheless, mere imitation is a somewhat dissatisfying approximation to programming from a computer-scientific perspective -- lacking in some essential aspect the qualities its practitioners aspire to fulfill. Helpful though it may be for tedious chores, the art of programming is not reading gigabytes of code and minimizing a cross-entropy loss. Programming requires imagination, creativity, problem-solving -- qualities which cannot be conferred by simply scaling existing language models with more data and parameters. What could be missing?

\section{Knowledge Discovery and Neural Code Search}

For decades, computer scientists have pondered the nature of search. Search is an indispensable tool in the working programmer's repertoire and goes to the heart of many fundamental problems in artificial intelligence, from classical to statistical optimization, and information retrieval to computational linguistics. Programming itself can be seen as a kind of search-based optimization problem~\cite{alur2018search}, consistent with its original mathematical interpretation, e.g. linear or stochastic programming. Coupled with a grammatical template, one could imagine searching through the space of valid programs to produce a higher-order function satisfying some criteria. Indeed, this exact setup is routinely-studied in the annual syntax-guided synthesis (SyGuS) competition~\cite{alur2016sygus}.

Returning to our earlier question of, ``What else could human programmers be doing besides imitation learning?'', one plausible answer could be trial and error. Given some program specification and a computational budget, a na√Øve strategy could be to simply evaluate as many programs from a dataset of candidate solutions as possible within the allotted budget. Many programmers do in fact practice this style of copy-paste programming as evidenced by duplicate code studies~\cite{lopes2017dejavu}, a problem known to adversely bias machine learning models, and which must be corrected for during data curation~\cite{allamanis2019adverse}.

For nearly all practical programming languages, the space of valid programs can hardly be enumerated, never mind evaluated in a reasonable amount of time. A more refined strategy is needed: for example, we could select a small set of reusable building blocks, then compose and evaluate partial programs using an execution-guided scheme~\cite{chen2018execution, wang2018execution}. By interacting with an interpreter, we may be able to arrive at a solution via incremental improvement. As with most dynamic programming algorithms, the problem comes down to a question of substructure: if modification invalidates prior work, search can become exponential or worse.

For example, many useful programs belong to the class of context free languages. Sampling is possible using a probabilistic grammar, but at what cost? The number of distinct parse trees grows super-exponentially with height. Various strategies have been designed to inhibit this growth, but even with judicious pruning, the topology of many languages does not admit an efficient beam search, or the cost required may be prohibitive. Yet humans are able to solve many computationally hard search problems with deceptive ease. How?

One possibility is that humans possess more computational resources than might we give them credit for, and a similarly-enriched neural-guided search would be equally capable. Another hypothesis is that we are not \textit{searching} for programs per se: when painting a portrait or writing a novel, we do not call this search. Likewise, programming is not necessarily about searching for an answer, but finding the right question, of exploring a design space which has no specification, or whose specification is a consequence, not a catalyst of the design. The program itself could merely be an artifact of an ephemeral dialog between a human and a machine, not the end result. In the following sections, we explore two contrasting models for this dialog, one where the human is the teacher~(\S\ref{sec:automatic-and-declarative-programming}), and one where the IPT is the teacher~(\S\ref{sec:computer-aided-reasoning-tools}).

  \section{Declarative and Automatic Programming}\label{sec:automatic-and-declarative-programming}

In The Art of Computer Programming~\cite{knuth1997art}, Donald Knuth memorably writes, ``Programs are meant to be read by humans and only incidentally for computers to execute.'' Taking this perspective, one may be tempted to ask, ``Why must programming languages be so difficult that we need IPTs to write down our ideas in the first place?'' If we consider programming to be simply a matter of communicating human intent to machines, language designers should take great pains to simplify the language so that users may convey their intent in an effortless manner, then harness machine intelligence in the service of fulfilling that intent, rather than force the user to describe exactly how it should be implemented. Known as \textit{declarative programming}, this approach can be found in popular languages like SQL, Prolog, miniKanren, and others.

The essence of declarative programming can be traced back to the 1940s when researchers started applying tools from mathematical optimization to what is today known as operations research~\cite{kantorovich1960mathematical}. In this early work, programmers would first state their intent as a solution to a system of inequalities, such as a linear or quadratic program, e.g. for optimal transport or economic planning. More recently, it has found important applications for defining metrics on graphs and probability distributions, the construction of which are an essential aspect of machine learning. Metrics provide a means of biasing learning algorithms towards solutions of a certain form, and thus can be seen as a mechanism for indirectly ``programming'' machine learning models.

Not only can declarative programming be used to produce numerical solutions to systems of equations, but the same ideas can be applied to synthesize other kinds of programs. Given some existing program $\mathcal P$ and a set of declarative constraints in the form of program transformations, we could construct an \textit{adjoint} program $\mathcal P'$ by applying the transformations recursively. For example, this might allow us to compute a density function by sampling outputs from the program probabilistically, or compute its derivative with respect to one or more inputs. This set of techniques is broadly known as \textit{automatic programming}.

Our primary interest in automatic programming is twofold: (1) as the basis for the first successful open-source implementation of gradient-based learning~\cite{baydin2018automatic}, and (2) as a practical framework for realizing the once-scorned~\cite{dijkstra1979foolishness} but now increasingly plausible~\cite{chen2021evaluating} idea of natural language programming. For example, consider the following natural language specification of an incomplete function:

\begin{lstlisting}[basicstyle=\small\ttfamily]
  fun transformStringIntoListOfUniqueWords(s: String) = TODO()
\end{lstlisting}

\noindent This is the completion provided by Codex~\cite{chen2021evaluating}, a recent IPT from OpenAI:

\begin{lstlisting}[basicstyle=\small\ttfamily]
  fun transformStringIntoListOfUniqueWords(s: String) =
     s.toLowerCase().replace(Regex("[^a-zA-Z]"), " ").trim()
     .split("\\s+".toRegex()).filter { it.isNotBlank() }.toSet()
\end{lstlisting}

\noindent The problem with automatic programming is that it assumes the specification is (1) infallible and (2) faithfully represents its author's intent. If either assumption fails, the result could be nonsensical, or appear to work but contain a subtle error. How could we provide earlier feedback if the specification were ill-posed?

\section{Computer-Aided Reasoning Tools}\label{sec:computer-aided-reasoning-tools}

As adept as human programmers are at certain tasks, they can be surprisingly short-sighted at others. The larger a program grows, the more likely its author is to make a mistake due to combinatorial explosion. This is why simplicity is so highly prized in language design - whenever a new feature is added, it has the potential to interact with every other feature in unpredictable ways, a challenge which has come to be known as the feature-interaction problem~\cite{apel2013exploring}. Taking this perspective, the programmer is not an all-knowing oracle, but actually a student who proposes ideas to the IPT, which returns feedback about their consequences within in the context of a \textit{type system}.

When programmers ask for an intelligent programming tool, what they really want is not a subordinate who blindly follows orders, but a teacher who rapidly gives them feedback about the implications of their design choices within the constraints of a well-defined language. This is the advantage of having a type system: not only does it give relevant feedback, but allows us to constrain the space of semantically valid programs. The interaction model is bidirectional: the user provides a typed program sketch. If the program is valid, the type is witnessed by an inhabitant. And if the program is inconsistent, the type system provides feedback to the user as to where and how.

For example, all procedures in a statically-typed programming language have a \textit{type signature}, for example, \texttt{getHomePhone: Person ‚Üí PhoneNumber}. This represents a \textit{contract} between the procedure and its caller: if the procedure \texttt{getHomePhone} is called with a \texttt{Person}, it will return a \texttt{PhoneNumber}. All other inputs and outputs are forbidden by the type system.

What if we could encode the type system constraints into our neural language model - how could we do that? One way is to use neural guided search, where we do beam search over a generative language model and discard the programs which are not well-typed. The second way is more difficult, but the morally ``correct'' solution: we learn a probability distribution on the space of semantically correct programs from the very outset, so the result is always guaranteed to be valid and search becomes unnecessary. Broadly speaking, this approach is known as type-safe probabilistic programming, and has been considered in various settings~\cite{murali2017bayesian}.

\pagebreak\section{Semiring Programming}\label{sec:semiring-programming}

By considering different algebras, we can compute different properties on graphical objects. These things are called algebraic path problems and have many useful applications in combinatorial optimization.

It is possible to implement many fundamental computer science algorithms in a much simpler way as iterated matrix multiplication on a semiring algebra. A commutative monoid $(S, ‚Ä¢, \circled{1})$ is a set $S$ with a binary operator $‚Ä¢: S \times S ‚Üí S$ which has the following additional properties:

  \begin{prooftree}
    \bottomAlignProof
    \AxiomC{$a ‚Ä¢ (b ‚Ä¢ c)$}
    \UnaryInfC{$(a ‚Ä¢ b) ‚Ä¢ c$}
    \noLine
    \UnaryInfC{}
    \noLine
    \UnaryInfC{\textit{Associativity}}
    \DisplayProof
    \hskip 2.5em
    \bottomAlignProof
    \AxiomC{$a ‚Ä¢ \circled 1$}
    \UnaryInfC{$a$\vphantom{$()$}}
    \noLine
    \UnaryInfC{}
    \noLine
    \UnaryInfC{\textit{Neutrality}}
    \DisplayProof
    \hskip 2.5em
    \bottomAlignProof
    \AxiomC{$a ‚Ä¢ b$}
    \UnaryInfC{$b ‚Ä¢ a$\vphantom{$()$}}
    \noLine
    \UnaryInfC{}
    \noLine
    \UnaryInfC{\textit{Commutativity}}
  \end{prooftree}

A semiring algebra, denoted $(S, \oplus, \otimes, \circled{0}, \circled{1})$, is a set together with two binary operators $\oplus, \otimes: S \times S ‚Üí S$ such that $(S, \oplus, \circled{0})$ is a commutative monoid and $(S, \otimes, \circled{1})$ is a monoid. It has the following additional properties:

  \begin{prooftree}
    \bottomAlignProof
    \AxiomC{$a \otimes (b \oplus c)$}
    \UnaryInfC{$(a \otimes b) \oplus (a \otimes c)$}
    \noLine
    \UnaryInfC{}
    \AxiomC{$(a \oplus b) \otimes c$}
    \UnaryInfC{$(a \otimes c) \oplus (b \otimes c)$}
    \noLine
    \UnaryInfC{}
    \noLine
    \BinaryInfC{\textit{Distributivity}}
    \DisplayProof
    \hskip 2.5em
    \bottomAlignProof
    \AxiomC{$a \otimes \circled 0$}
    \UnaryInfC{$\circled 0$\vphantom{$()$}}
    \noLine
    \UnaryInfC{}
    \noLine
    \UnaryInfC{\textit{Annihilation}}
  \end{prooftree}

It is possible solve a rich family of optimization problems using iterated matrix multiplication on semirings. Known as \textit{propagation} or \textit{message passing}, this procedure consists of two steps: \textit{aggregate} and \textit{update}. Let $Œ¥_{st}$ denote some distance metric on a path between two vertices $s$ and $t$ in a graph. To obtain $Œ¥_{st}$, one may run the following procedure on a desired path algebra until convergence (a few examples are provided for illustration):

  \begin{center}
    \begin{tabular}{lc|cr}
      $Œ¥_{st} = \overbrace{\underset{P\in P_{st}^*}{\bigoplus}\underbrace{\underset{e\in P}{\bigotimes}W_{e}}_{\text{Aggregate}}}^{\text{Update}}$ & & &
      \bgroup
      \def\arraystretch{1.2}
      \begin{tabular}{c|c{1cm}c{1cm}|c{1cm}c{1cm}|c}
        S                           & $\oplus$ & $\otimes$ & $\circled{0}$ & $\circled{1}$ & Path     \\\hline
        $\mathbb R \cup \{\infty\}$ & min      & +         &   $\infty$    &      0        & Shortest \\
        $\mathbb R \cup \{\infty\}$ & max      & +         &   $-\infty$   &      0        & Longest  \\
        $\mathbb R \cup \{\infty\}$ & max      & min       &       0       &   $\infty$    & Widest   \\
      \end{tabular}
      \egroup
    \end{tabular}
  \end{center}

  \noindent Many dynamic programming algorithms, including Bellman-Ford, Floyd-Warshall, Dijkstra, belief, constraint, error and expectation propagation, Markov chains, and others can all be neatly expressed as message passing on a semiring algebra. We refer the curious reader to~\cite{gondran2008graphs,baras2010path} for a more complete summary of the algebraic path problem and its many wonderful applications.


\noindent\section{Future Directions of Computer Programming}

\textit{Procedural knowledge} is knowledge designed or discovered to facilitate a process. Unlike \textit{relational knowledge} which may be bidirectional, procedural knowledge has a specific direction or goal. The methods presented in this literature review have broad applications to parsing and indexing procedural knowledge. Given a sequence of \textit{paths}, or \textit{traces} representing discrete steps in a goal-directed process with intermediate results, our research seeks to provide a mechanism for estimating procedural similarity, to synthesize or recommend semantically similar procedures under feasible reorderings. Specifically, our work focuses on the domain of \textit{programming} knowledge.

Code shares many statistical properties in common with natural language and can be studied using natural language processing~\cite{hindle2012naturalness}. Unlike natural language, all syntactically-valid code has an unambiguous grammar. Furthermore, code primarily consists of \textit{functions} or \textit{procedures} intended to operate a machine. The underlying programming model may be relational, object-oriented or functional, but all mainstream programming languages support some form of procedure that accepts inputs and produces outputs.


Application programming interfaces (APIs) are a set of interfaces which describe available ways of structuring computation to achieve a set of related programming tasks. We call the graph of all possible ways to compose an API the \textit{API surface}. One traverses the API surface by composing accessible procedures in a \textit{call graph}. Thus, we can view the API as kind of a \textit{procedural knowledge base} representing common data transformations and how to compose them. In practice, how to achieve some desired goal is often far from obvious, requiring a large amount of documentation to explain.

Many consumers of popular APIs publish code and documentation in open source repositories, a largely untapped source of knowledge for programming tools. New work seeks to find ways of linking knowledge contained in open source repositories to help users locate examples and compose software applications. In the following section, we propose three applications of procedural pattern recognition for documentation alignment (\S~\ref{subsec:tracelink}), code search (\S~\ref{subsec:code-search}), program repair (\S\ref{subsec:code-search}), and eDSL generation (\S~\ref{subsec:code-search}).

\pagebreak\subsection{Documentation alignment}\label{subsec:tracelink}

Documentation is an indispensable resource for software developers learning to use a new API or programming language. Maintainers of popular software projects often publish web-based developer documents, typically in markup languages such as HTML or Markdown. These documents contain a mixture of natural language sentences, code snippets, and hyperlinks to related documents and source code files. All of these artifacts hold rich semantic information: the markup graph describes the text in relation to other entities in the document hierarchy, while the link graph describes relationships between relevant documents or artifacts in a software repository.

Consider the typical workflow of a software developer who is seeking information about an unfamiliar API. To effectively locate relevant documentation, she must first copy a specific fragment of text (e.g. a function name, error message, or identifier) from a development environment into a search engine, providing relevant contextual information. The query must be descriptive enough to retrieve relevant documents with high probability, while omitting extraneous information (e.g. user-defined tokens) unlikely to occur outside the scope of the developer's personal environment or project.

Prior work in information retrieval for software development investigated recommending API documentation~\cite{robillard2015recommending} and Q\&A content~\cite{treude2016augmenting} to developers. Similar work in natural language processing has studied the relationship between comments and source code entities~\cite{iyer2018mapping, panthaplackel2020associating} strictly within source code. Examples of cross-domain entity linking in the source-to-doc (S2D) and doc-to-source (D2S) setting are scarce, however these results indicate alignment between natural language and software artifacts may be feasible.

New work seeks to facilitate procedural knowledge discovery by enriching lexical queries with semantic information extracted from a programming environment, and prioritizing semantically relevant software artifacts among a set of matching search results. Broadly, the tools we have proposed in this literature review can be used to study both source code and procedural knowledge graphs, however due to the paucity of cross-domain entity links between code and natural language artifacts, reasoning about cross-domain relations will require developing new approaches to feature engineering, unsupervised learning and entity alignment in the low-data regime.

Such an application, if successful, would allow developers to more quickly and easily locate semantically or contextually relevant code samples in API documentation and open source repositories. The same tools could also help authors maintain a consistent set of API documentation and usage examples across a large codebase, a persistent obstacle when evolving any API.

  \pagebreak\subsection{Code search}\label{subsec:code-search}

Given a learned similarity metric between procedures, one straightforward application is code search. Prior work in this area has explored type-directed~\cite{james2020digging}, learning-based~\cite{gu2018deep} and semantic search~\cite{premtoon2020semantic} techniques. These techniques all use a fixed, or synthetic ordering over search results. For a given hole context, there are often many valid completions within an API or codebase. Given a corpus of procedures in their surrounding typing environment, is it possible to estimate a probability distribution on a shared embedding between contexts and results, and measure the likelihood that a given search result occurs in an empty location? This requires:

  \begin{enumerate}
    \item Efficiently searching a corpus for a well-typed pattern
    \item Ranking the matching search results by semantic alignment
    \item Incorporating information into user's context (e.g. variable renaming)
  \end{enumerate}

Given a cursor and the surrounding context (e.g. in an IDE or editor), such a tool would need to search a database for the most similar contexts, extract common snippets to estimate their \textit{concordance} or \textit{agreement} with the surrounding code context, then adapt the foreign code snippet into to the user's context. External contexts including public code samples or API-documentation (e.g. fixes or repairs for compiler error messages) could help the user to complete some unfinished piece of code. Some open research questions include:

  \begin{enumerate}
    \item \textbf{Semantic segmentation}: How do we slice or truncate context?
    \item \textbf{Graph search}: What kernels enable fast subgraph detection?
    \item \textbf{Context ranking}: What features best measure contextual similarity?
    \item \textbf{Refactoring}: How to integrate a selected result into the user's code?
  \end{enumerate}

What is a language model? Disregarding meaning, a language model is no more than a statistical model of symbolic information. Natural languages differ from mechanical languages in a few key ways. Natural languages are linear: almost all human languages have a fixed maximum recursion depth governed by biological and cognitive factors. Mechanical languages, by contrast, and are typically nonlinear due to their metalinguistic properties. Thus, natural language models cannot represent a mechanical language without significantly constratining their expressiveness.

One might argue -- since natural language models are effectively linear -- a language model with a large enough working memory, given enough data, should be able to model language fragments whose description length fits inside their working memory. Indeed, recent literature has shown that natural language models are surprisingly adept at modeling idioms in source code. However, we would expect such models to struggle with fragments whose information complexity exceeds its working memory (e.g. trees and graphs whose average MDL stretch the limit).

What is the difference between working memory, i.e. reasoning, and long-term memory, i.e. knowledge? Long term memory is information stored in the learned parameters, i.e. the topological structure of the network -- as a model \textit{learns}, this information is passively encoded in the topological structure. Long-term mememories must be conserved by the data distribution. Distributional shift tends to erase past memories, a phenonomena known as catastrophic forgetting.

In contrast, short-term memories are dynamical patterns of \textit{activity} which must be actively consereved by the network topology. This second mechanism, called working memory, is a dynamical process with a much smaller information footprint, but allows a network to reason about incoming signals and adapt to previously unseen scenarios without memorization. To be passed on to successive layers, working memories must be actively consereved by the network topology. Both learning and reasoning are a forms of message passing over a graphical structure: learning shapes the graph topology, and reasoning propagates signals through it.

Models which cannot fit a language into working memory must learn maximal-length framgents and use long-term memory to fill in the gaps -- i.e. by memorizing transitions between fragments in long-term memory. If this is the case, we would expect to find transitions that are locally consistient, but globally nonsensical. For instance, can source code models learn balanced parenthesis? Balanced intermingled parenethesis? \texttt{\{([])\}} Other algebraic datastructures (e.g. bush, imbalanced trees, etc.)? What are the limit languages that can be learned by natural langauge models of source code (e.g. Transformers)?

Early work in program learning realized the importance of graph-based representations~\cite{allamanis2017learning}, however explicit graph construction requires extensive feature-engineering. More recent work in program synthesis has explored incorporating a terminal~\cite{ellis2019write}, graphical~\cite{walke2020learning} or other user interface to explore the space of valid programs, however do not consider the scope or variety of artifacts in a software project. Others have shown the feasibility of learning a local graph~\cite{johnson2020learning} from source code, but still require an explicit parser to form the initial graph and adaption to settings where style, terminology and document structure vary remains a challenge.

Imagine a newly-hired developer, who has programming experience, but no prior knowledge about a closed-source project. She receives access to the team's Git repository and is assigned her first ticket: Fix test broken by \texttt{0fb98be}. After locating the commit and becoming familiar with the code, she queries StackOverflow, discovers a relevant solution, copies the code into the project, makes a few edits, presses run, and the test passes.

Humans are adept information foraging agents. We can quickly find relevant information in a large corpus by recognizing and following textual landmarks. Software projects are composed of a variety of semi-structured documents containing many clues where relevant information may be found. In this work, we learn a grammar from a model trained on software artifacts like source code and documentation, in order to facilitate common programming tasks such as code search, completion, or knowledge discovery.

Early work in program learning realized the importance of graph-based representations~\cite{allamanis2017learning}, however explicit graph construction requires extensive feature-engineering. More recent work in program synthesis has explored incorporating a terminal~\cite{ellis2019write}, graphical~\cite{walke2020learning} or other user interface to explore the space of valid programs, however do not consider the scope or variety of artifacts in a software project. Others have shown the feasibility of learning a local graph~\cite{johnson2020learning} from source code, but still require an explicit parser to form the initial graph and adaption to settings where style, terminology and document structure vary remains a challenge.

Prior work in the code search literature explores the text-to-code~\cite{husain2019codesearchnet} setting, where queries are typically considered to be a short sequence composed by the user, or code-to-code~\cite{kim2018facoy} setting where the query takes the form of a code snippet. Model performance typically evaluated using mean reciprocal rank (MRR), mean average precision (MAP), normalized discounted cumulative gain (NDCG), precision and recall at top N-results (P/R@N), or similar metrics. Although some~\cite{asyrofi2020ausearch} do incorporate other features from the local document, none however, consider the query in the context of a broader project. The ability to align contextual features from the surrounding project, we argue, is essential to delivering semantically relevant search results. Prior studies of natural language in source code have been undertaken ~\cite{weiss2018practical, chirkova2020empirical, chen2021evaluating}, which characterize the families of computational languages that neural language models can recognize in practice.

The proposed method presents an interesting engineering challenge. To work for code completion, it would need to have relatively low latency for a pleasant user experience. While the model could be trained on a large corpus offline, the contextual embedding would need to be periodically recomputed as the surrounding typing environment changes in the editor. Furthermore, the search and retrieval speed would need keystroke latency to be effective.

  \subsection{DSL generation}\label{subsec:gen}

Most knowledge starts with pen on paper. It is generally possible for developers to translate well-structured pseudocode into code. However, there are many valid translations -- the same algorithm implemented in the same language by different authors is seldom written the same way. Instead of translating these ideas from scratch, it may be possible to encode just a few axioms and enough knowledge to derive a family of algorithms, reusing existing optimizations written by other developers and selecting the most appropriate procedure for computing a desired quantity, e.g. using a sufficiently expressive logic system, and a database of simple facts and relations.

Knowledge systems or \textit{ontologies} are a collection of related facts designed or discovered by human beings. For example, we can treat mathematical knowledge as a kind of symbolic rewrite system. This has been successfully operationalized in Theano~\cite{bergstra2010theano} and other DSLs. It is possible to view constructive mathematics libraries like Metamath~\cite{megill2006metamath}, Rubi~\cite{rich2009knowledge}, Arend, KMath~\cite{nozik2019kotlin} and Kotlin$\nabla$~\cite{considine2019kotlingrad} as also working towards this high-level goal.

In knowledge engineering, approximate equality is known as entity \textit{alignment} or \textit{matching}. With an approximate matching algorithm, we could accurately detect near-duplicates in a large codebase, find similar documentation snippets, retrieve contextually or semantically relevant code samples to assist developers writing unfamiliar code, and search for bugs in code or fixes from an external knowledge base to repair them. All of these tasks require some notion of semantic similarity between procedures.

  \section{Conclusion}

  \pagebreak \section{Acknowledgements}

The author wishes to thank his advisors Jin Guo and Xujie Si, for their feedback on this literature review, and his colleagues Disha Shrivastava and David Yu-Tung Hui, for various interesting conversations during the pandemic.

  \bibliography{literature_review}
  \bibliographystyle{plain}
\end{document}
%! Author = breandanconsidine
%! Date = 8/9/21

% Preamble
\documentclass[10pt]{article}

% Packages
\usepackage{amsmath}

% Document
\title{Programming in the Age of Intelligent Machines}
\author{Breandan Considine}
\date{\today}

\begin{document}
  \maketitle
  \section{Introduction}

Since the invention of modern computers in the mid 20th century, computer programming has undergone a number of paradigm shifts. From the rise of functional programming, to dynamic, object-oriented, and type-level programming, to the availability of myriad tools and frameworks -- its practitioners have witnessed a veritable Renaissance in the art of computer programming. With each of these paradigm shifts, programmers have realized new conceptual frameworks for reasoning and expressing their ideas more clearly and concisely.

Over the last few years, another paradigm shift has been set in motion, with significant implications for how we think about and write programs in the coming century. By most measures, computers have grown steadily more intelligent and capable of assisting programmers with mentally taxing chores. For example, intelligent programming tools (IPTs) powered by neural language models have this year helped over 10 million unique human beings program computers. As IPTs help digitally illiterate communities to discover their innate aptitude for computer programming, this population will continue to rise.

Computer programming is a uniquely creative exercise among the range of human activities. It channels our innate linguistic, logical, imaginative, and social abilities to bring abstract ideas into reality, and ultimately, gives humans the freedom to create new realities of their own design. In collaboration with other humans and the increasing participation of IPTs, vast and elaborate virtual worlds have been manufactured, where the majority of humankind now chooses to spend their daily lives. With the expanding opportunities these new digital frontiers promise, their population too will continue to grow.

Today IPTs share an equal role in shaping many aspects of computer programming, from knowledge discovery to API design, and program synthesis to validation and verification. However, this balance is shifting beneath our feet. Once its creators, programmers are now primarily consumers of information provided by an IPT, and increasingly rely on them to perform their daily work. With the unique opportunities and risks this partnership presents, what division of labor should exist between humans and our new coding collaborators? This is the question we have set out to understand in the following literature review.

  \section{Neural Language Models for Source Code}

Programming researchers have long held an interest in using intelligent tools to help them write programs~\cite{bras1993artificial}. Due to fundamental limitations in data and processing power, many of these ambitions did not come to fruition until the last few years, thanks the availability of \textit{big code}~\cite{allamanis2018survey}, the development of differentiable programming libraries for gradient-based learning~\cite{baydin2018automatic}, and attention-based language models~\cite{vaswani2017attention}, among other technical achievements. Armed with this new repertoire, programming researchers have revisited their interest in IPTs. Naturally, one of the first applications considered was code completion.

 Following their initial success in natural language, rapid progress continues to be made in the design and application transformers to source code, as well as industrial transfer where this technology is now trained and deployed on millions of programmers worldwide~\cite{chen2021evaluating}. Given a natural language description of an incomplete method, these models are capable of inferring programmer intent and completing multiline code snippets. Progress is expected to improve.

The problem comes down to a question of grammar induction. Based on empirical results, fixed-precision transformers (e.g. GPT-2, BERT) are thought capable of recognizing the class of counter languages~\cite{bhattamishra2020ability}, i.e. somewhere between context-free and context-sensitive, although this characterization requires a more careful theoretical analysis. For source code typically stored on GitHub, this class would appear to suffice -- models trained on such datasets are currently capable of rudimentary program sketching and boilerplate code completion, however more complex fragments require additional oversight.

An important shortcoming of imitation learning is the question of data provenance and validation. Even if the training data is syntactically and semantically valid, constraints on the class of valid programs are ill-posed. As a consequence, a large fragment of syntactically valid programs that may be generated are semantically unsound, i.e. may throw runtime errors at best, or appear to work at first glance, but are in fact broken in a subtle manner. Like most language models of its kind, performance is highly sensitive to the dataset quality, as common errors in the training data can be inherited and reproduced.

The vast majority of modern programming consists of writing ceremonial boilerplate, tasks for which neural language models are well-suited. A tremendous amount of human labor is spent on such chores, and reallocating those resources towards more intellectually stimulating tasks may encourage a larger demographic to become programmers who otherwise lack the patience or interest. By removing these barriers to entry, programmers can more quickly arrive at the rewarding parts of program design and implementation.

Nevertheless, mere imitation is a somewhat dissatisfying approximation to programming from a computer-scientific perspective -- lacking in some essential aspect the qualities its practitioners aspire to fulfill. Helpful though it may be for tedious chores, programming is not about reading gigabytes of code and minimizing a cross-entropy loss. Programming requires imagination, creativity, problem-solving -- qualities which cannot be conferred by simply scaling up language models with more data and parameters. What could be missing?

  \section{Knowledge Discovery and Neural Code Search}

Search is an indispensable tool in computer programming. Many problems in computer science can be distilled to search, and the subject is widely studied in dynamic programming, statistical learning, and computational linguistics.

Regarding, ``What else could programmers be doing besides imitation?'', one plausible answer could be trial and error: given some program specification and a computational budget, a novice programmer could simply evaluate as many programs as possible in resources allotted. In what order should candidate programs be evaluated? Many heuristics have been proposed, from execution-guided search~\cite{chen2018execution, wang2018execution}, to weighted search using reinforcement learning, to other dynamic programming strategies. These all rely on judicious pruning of the search space.

Many problems we want to solve require searching a space whose complexity is simply inaccessible. Various mitigations such as backtracking and random restarts have been proposed to alleviate this issue, however the topology of many search problems does not admit an efficient beam search.

  \section{Automatic and Synthetic Programming}



  \section{Computer-Aided Reasoning Tools}

When programmers ask for an intelligent programming tool, what they really want is not a subservient genie who grants wishes without question, but a teacher who rapidly gives them feedback about the implications of their design choices within the confines of a well-defined language. This is one advantage of a type system: while not particularly intelligent, it is at least reliable.

  \section{Future Directions of Computer Programming}

  \section{Conclusion}

  \bibliography{literature_review}
  \bibliographystyle{plain}
\end{document}
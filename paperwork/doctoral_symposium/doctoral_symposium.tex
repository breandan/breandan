\documentclass[sigplan,screen]{acmart}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\usepackage{amsmath}

\begin{document}

\title{Syntax Repair as a Language Game}

\author{Breandan Considine}
%\authornote{Both authors contributed equally to this research.}
\email{bre@ndan.co}
\affiliation{%
  \institution{McGill University}
}

%\author{Lars Th{\o}rv{\"a}ld}
%\affiliation{%
%  \institution{The Th{\o}rv{\"a}ld Group}
%  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%  \city{Hekla}
%  \country{Iceland}}
%\email{larst@affiliation.org}
%
%\author{Valerie B\'eranger}
%\affiliation{%
%  \institution{Inria Paris-Rocquencourt}
%  \city{Rocquencourt}
%  \country{France}
%}


\renewcommand{\shortauthors}{Considine}

%\begin{abstract}
%  A clear and well-documented \LaTeX\ document is presented as an
%  article formatted for publication by ACM in a conference proceedings
%  or journal publication. Based on the ``acmart'' document class, this
%  article presents and explains many of the common variations, as well
%  as many of the formatting elements an author may use in the
%  preparation of the documentation of their work.
%\end{abstract}

\maketitle

\section{Abstract}

Programming languages share a social and formal heritage. These families were historically divided, but share deep roots, and we argue their destined matrimony heralds important consequences for language design and generative langauge modeling. In our work, we develop a sociotechnical framework for understanding the dynamics of programming and argue it captures many of the social and formal properties of language acquisition and evolution.

\section{Motivation}

%Why do we care about the problem and the results? If the problem isn’t obviously interesting it might be better to put motivation first, but if your work is incremental progress on a problem that is widely recognized as important, then it is probably better to put the “Problem” section first to indicate which piece of the larger problem you are breaking off to work on. This section should include the importance of your work, the difficulty of the area, and the impact it might have if successful.

%Through the millennia, human beings have sought ways to mechanize the process of thought. From the abacus to the modern computer, we have constructed increasingly sophisticated systems to augment our cognitive abilities. In the 21st century, we are witnessing the emergence of a new class of intelligent machines, which are capable of learning from experience and solving problems in ways that are difficult to express in code. These machines are known as \emph{artificial neural networks} (ANNs), and they are the subject of intense research in the fields of computer science and statistics.

Are languages acquired or constructed? In empirical software engineering and sociolinguistics, many would argue languages are statistical phenomena that arise in the context of repeated social interactions. Tracing linguistic artifacts can shed light into language acquisition and adaptation, and offers insight into the social dynamics of communicating agents. In contrast, programming languages are deliberately constructed and most designers would argue a language's social dynamics, though perhaps informative, are surely less fundamental than its runtime dynamics. This is a curious distinction, since programming languages appear to share many statistical similarities with natural language~\cite{hindle2016naturalness}, and the act of programming can itself be understood as a form of communication~\cite{demillo1979social} between minds and machines.

Our research seeks to understand the common principles that shape the creation and evolution of natural and programming language artifacts, informed by cooperative game theory. In particular, we draw inspiration from both natural and computational linguistics to develop a constructive theory of code completion and program repair. Our proposed framework can be translated into practical development tools for software engineering, as well a benchmarking suite for probing the reasoning capabilities of neural language models on prosocial programming problems (PPPs).

Language games are a pragmatic framework for modeling the social dynamics of code. In one such game, two players, the driver and a navigator, collaborate across a shared workspace to construct a domain-specific language that solves a chosen problem. Initially, both parties have a limited understanding of their counterpart and to reach a common understanding, must exchange messages conveying, e.g., their intent, abilities and knowledge. This game comes to an end when the chosen problem is successfully solved, the clock eventually runs out, or the problem is deemed unsolvable and one player forfeits due to exhaustion.

We call this framework \emph{pragmatic alignment} and argue it not only encompasses many behavioral aspects of social computing such as hackathons and pair programming, but also underpins the design and implementation of integrated development environments (IDEs), and offers a benchmark for evaluating the systematic reasoning and compositional generalization of intelligent programming tools (IPTs). In light of our growing reliance upon IPTs and their evident limitations, new techniques are needed to increase the reliability of programming agents deployed in prosocial environments.

No more evident is the rudimentary state of programmatic reasoning than the currently popular alignment mechanism, which uses imitation learning with human feedback to fine-tune a transformer-based language model, then employs a crude prompt like, ``Let's think step-by-step\ldots''. While it may convincingly imitate humans on certain programmatic tasks, this strategy is hopelessly flawed, as (1) human feedback is sparse and often incorrect, (2) transformers often learn reasoning~\cite{dziri2023faith} shortcuts~\cite{liu2022transformers} that fail to generalize across domains, and (3) transformers have known limitations~\cite{merrill2022saturated} to their expressive~\cite{chiang2023tighter} capacity preventing generalization, even in the infinite limit of data and compute. These shortcomings raise serious doubts that preference alignment alone can produce programming agents capable of solving decision problems whose expected complexity is significantly harder-on-average than presented in the training distribution.

Instead, the pragmatic approach advocates for training and evaluating programmatic reasoners on a curriculum of language games with increasing average-case complexity, such as proof search or program repair. As we argue, logical or semantic validity is a more trustworthy alignment criteria than preferability, and a much more reliable indicator of systematic and compositional generalization -- the hallmarks of programmatic reasoning in humans. Pragmatism can also complement preference alignment to improve the reliability and preferability of generated source code, and offers a more principled approach to designing and implementing IDEs.

To anchor our discussion hereinafter, we shall restrict ourselves to a specific game exemplifying the pragmatic framework that falls at the intersection of programming languages and developer tools: syntactic error correction. Towards that end, we develop a tool for correcting invalid syntax and evaluate it on a variety of toy and practical programming languages, demonstrating SoTA precision and throughput at a fixed timeout relative to transformers. Tidyparse, as we call it, is useful in its own right for program repair, and provides a benchmarking suite for evaluating programming agents.

\section{Problem}

%What exact problem, issue, or question does this research address? What limitations or failings of current understanding, knowledge, methods, or technologies does this research resolve? You should position your work with respect to related ideas in this section.

Broadly put, program repair is the problem of refining an existing program that approximates some specification, but falls short. In this regard, repair is a bit like a limit-process: the more we refine our code, the closer it should become to its specification. The pragmatic approach to program repair takes a charitable view of the author: not as a clumsy fool to be scolded into conformity, but an earnest programmer trying to convey their intent, who may have strayed slightly off-course. The pragmatic ideal tries to imagine what the author could have intended, as opposed to prescribing what should have been written. We will consider a very narrow form of program repair, namely \emph{syntax correction}.

When writing code, nearly all the intermediate editing states are syntactically invalid. Manual repair, though fairly routine, introduces friction to the development process by occasionally diverting the author's attention. \textit{Syntax correction} is the process of automatically repairing a syntactically invalid program so that it is no longer invalid. Trivial though it may seem, this problem can be quite challenging, as well-formed programs have many semantic constraints, but also because the solutions are highly under-determined: although repairs must be valid, even assuming a tiny number of edits, the search space of valid repairs can be vast indeed.

As specified, this particular game has a few other rules that we self-impose: the solver must seek out plausible repair(s) first, but eventually discover every syntactically valid repair within a certain number of edits from the original invalid source code. Furthermore, the solver must sample repairs without replacement in parallel across all available cores, and whensoever interrupted, return the best candidates hitherto discovered. Finally, it must be able to repair programs in a variety of languages without any prior statistical knowledge. These constraints introduce a number of interesting design challenges, which we will now proceed to address.

\section{Approach}

%How did you go about solving or making progress on the problem? What new understanding, knowledge, methods, or technologies will this research generate?

We can reframe syntax error correction as a language edit distance problem, but instead of using the typical approach that only returns the closest repair(s) by edit distance, we try to find every single edit within a fixed distance. This problem can be modeled as a conjunctive language membership query between a set of context-free grammars and a Levenshtein automaton. At a high level, we take the intersection of the languages recognized by the grammars, encode them as a conjunctive language and enumerate every single edit within a fixed distance, then rank them by a suitable distance metric such as Levenshtein or finger-travel distance on a keyboard.

Crucially, this must be extremely fast, as the usefulness of our repairs are inversely proportional to the time it takes to generate them. To enable low-latency repairs, we develop a novel reduction from conjunctive language reachability onto finite field arithmetic, based on the idea of matrix multiplication with a logical semiring. Context-free language recognition can be reduced to iterated Boolean matrix multiplication. This representation is highly efficient from a complexity-theoretic standpoint, allowing us to solve for the missing entries using an idempotent matrix completion algorithm, and the resulting valid solutions can be reranked by the distance metric, then presented to the user for review.

\section{Evaluation Methodology}

%In writing the evaluation methodology section of your submission, we encourage you to emphasize two main aspects of your experiment:

There are essentially two ways to evaluate a syntax error corrector: one is to measure the rate at which valid repairs are discovered, and the other is to measure the rate at which human repairs are discovered. The former approach is technically correct, but since most parsers are relatively permissive, will admit a variety of repairs that are syntactically valid but otherwise unnatural or practically useless.

The latter evaluation considers both human preferences and syntactic validity -- human repairs are more challenging to discover, but arguably the more useful metric in practice. Although pragmatism generally favors the latter approach, preferentiality can be difficult to evaluate for minority languages and a potentially misleading metric when faced with out-of-distribution errors. We will therefor consider the former method to evaluate throughput, soundness and completeness of the solver, and the latter to measure its precision and latency.

We evaluate our solver on a variety of real-world programming languages like Python, Java and Kotlin, and compare it with a transformer-based language model trained on those same languages.

\subsection{Hypothesis}

%What would be the main research result? What would be the secondary research results? Phrase these as primary and secondary hypotheses.

We hypothesize that we can achieve a higher precision and throughput than transformers at a fixed timeout, and furthermore can do so without any prior statistical knowledge of the language.

\subsection{Evaluation Setup}

%How are you going to set up your experiments or studies to test these hypotheses? How do you plan to control for bias? How will you maximize external validity?

Using a pairwise dataset of human syntax errors in natural source code and human repairs from snippets StackOverflow, we preprocess the data to include only syntax pairs containing malformed code that are truly rejected by the parser, and repairs that are accepted by the parser. This dataset is used for the evaluation.

%bibliography:
\bibliography{doctoral_symposium}
\bibliographystyle{ACM-Reference-Format}
\end{document}
\endinput
\documentclass[sigplan,screen]{acmart}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\usepackage{amsmath}

\begin{document}

\title{A Pragmatic Approach to Syntax Repair}

\author{Breandan Considine}
%\authornote{Both authors contributed equally to this research.}
\email{bre@ndan.co}
\affiliation{%
  \institution{McGill University}
}

%\author{Lars Th{\o}rv{\"a}ld}
%\affiliation{%
%  \institution{The Th{\o}rv{\"a}ld Group}
%  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%  \city{Hekla}
%  \country{Iceland}}
%\email{larst@affiliation.org}
%
%\author{Valerie B\'eranger}
%\affiliation{%
%  \institution{Inria Paris-Rocquencourt}
%  \city{Rocquencourt}
%  \country{France}
%}


\renewcommand{\shortauthors}{Considine}

%\begin{abstract}
%  A clear and well-documented \LaTeX\ document is presented as an
%  article formatted for publication by ACM in a conference proceedings
%  or journal publication. Based on the ``acmart'' document class, this
%  article presents and explains many of the common variations, as well
%  as many of the formatting elements an author may use in the
%  preparation of the documentation of their work.
%\end{abstract}

\maketitle

\section{Abstract}

Programming languages share a social and formal heritage. These families were historically divided, but share deep roots, and we argue their destined matrimony heralds important consequences for language design and generative language modeling. In our work, we develop a sociotechnical framework for understanding the dynamics of programming and argue it captures many of the social and formal properties of language acquisition and evolution.

\section{Motivation}

%Why do we care about the problem and the results? If the problem isn’t obviously interesting it might be better to put motivation first, but if your work is incremental progress on a problem that is widely recognized as important, then it is probably better to put the “Problem” section first to indicate which piece of the larger problem you are breaking off to work on. This section should include the importance of your work, the difficulty of the area, and the impact it might have if successful.

%Through the millennia, human beings have sought ways to mechanize the process of thought. From the abacus to the modern computer, we have constructed increasingly sophisticated systems to augment our cognitive abilities. In the 21st century, we are witnessing the emergence of a new class of intelligent machines, which are capable of learning from experience and solving problems in ways that are difficult to express in code. These machines are known as \emph{artificial neural networks} (ANNs), and they are the subject of intense research in the fields of computer science and statistics.

Are languages acquired or constructed? In empirical software engineering and sociolinguistics, many would argue languages are statistical phenomena that arise in the context of repeated social interactions. Tracing linguistic artifacts can shed light into language acquisition and adaptation, and offers insight into the social dynamics of communicating agents. In contrast, programming languages are deliberately constructed and most designers would argue a language's social dynamics, though perhaps informative, are surely less fundamental than its runtime dynamics. This is a curious distinction, since programming languages appear to share many statistical similarities with natural language~\cite{hindle2016naturalness}, and the act of programming can itself be understood as a form of communication~\cite{demillo1979social} between minds and machines.

Our research seeks to understand the common principles that shape the creation and evolution of natural and programming language artifacts, informed by cooperative game theory. In particular, we draw inspiration from both natural and computational linguistics to develop a constructive theory of code completion and program repair. Our proposed framework can be translated into practical development tools for software engineering, as well as a benchmarking suite for probing the reasoning capabilities of neural language models on prosocial programming problems (PPPs).

Language games are a pragmatic framework for modeling the social dynamics of code. In one such game, two players, the driver and a navigator, collaborate across a shared workspace to construct a domain-specific language that solves a chosen problem. Initially, both parties have a limited understanding of their counterpart and to reach a common understanding, must exchange messages conveying, e.g., their intent, abilities and knowledge. This game comes to an end when the chosen problem is successfully solved, the clock eventually runs out, or the problem is deemed unsolvable and one player forfeits due to exhaustion.

We call this framework \emph{pragmatic alignment} and argue it not only encompasses many behavioral aspects of social computing such as hackathons and pair programming, but also underpins the design and implementation of integrated development environments (IDEs), and offers a benchmark for evaluating the systematic reasoning and compositional generalization of intelligent programming tools (IPTs). In light of our growing reliance upon IPTs and their evident limitations, new techniques are needed to increase the reliability of programming agents deployed in prosocial environments.

No more evident is the rudimentary state of programmatic reasoning than the currently popular alignment mechanism, which uses imitation learning with human feedback to fine-tune a transformer-based language model, then employs a crude prompt like, ``Let's think step-by-step\ldots''. While it may convincingly imitate humans on certain programmatic tasks, this strategy is hopelessly flawed, as (1) human feedback is sparse and often incorrect, (2) transformers often learn reasoning~\cite{dziri2023faith} shortcuts~\cite{liu2022transformers} that fail to generalize across domains, and (3) transformers have known limitations~\cite{merrill2022saturated} to their expressive~\cite{chiang2023tighter} capacity preventing generalization, even in the infinite limit of data and compute. These shortcomings raise serious doubts that preference alignment alone can produce programming agents capable of solving decision problems whose expected complexity is significantly harder-on-average than presented in the training distribution.

Instead, the pragmatic approach advocates for training and evaluating programmatic reasoners on a curriculum of language games with increasing average-case complexity, such as proof search or program repair. As we argue, logical or semantic validity is a more trustworthy alignment criterion than preferability, and a much more reliable indicator of systematic and compositional generalization -- the hallmarks of programmatic reasoning in humans. Pragmatism can also complement preference alignment to improve the reliability and preferability of generated source code, and offers a more principled approach to designing and implementing IDEs.

To anchor our discussion hereinafter, we shall restrict ourselves to a specific game exemplifying the pragmatic framework that falls at the intersection of programming languages and developer tools: syntactic error correction. Towards that end, we develop a tool for correcting invalid syntax and evaluate it on a variety of toy and practical programming languages, demonstrating SoTA precision and throughput at a fixed timeout relative to transformers. Tidyparse, as we call it, is useful in its own right for program repair, and provides a benchmarking suite for evaluating programming agents.

\section{Problem}

%What exact problem, issue, or question does this research address? What limitations or failings of current understanding, knowledge, methods, or technologies does this research resolve? You should position your work with respect to related ideas in this section.

Broadly put, program repair is the problem of refining an existing program that approximates some specification but falls short. In this regard, repair is a bit like a limit-process: the more we refine our code, the closer it should become to its specification. The pragmatic approach to program repair takes a charitable view of the author: not as a clumsy fool to be scolded into conformity, but an earnest programmer trying to convey their intent, who may have strayed slightly off-course. The pragmatic ideal tries to imagine what the author could have intended, as opposed to prescribing what should have been written. We will direct our attention to one particular form of program repair, namely \emph{syntax correction}.

When writing code, nearly all the intermediate editing states are syntactically invalid. Manual repair, though fairly routine, introduces friction to the development process by occasionally diverting the author's attention. \textit{Syntax correction} is the process of automatically repairing a syntactically invalid program so that it is no longer invalid. This problem may seem trivial but can be quite challenging, as well-formed programs have many semantic constraints, but also because the solutions are highly under-determined: although repairs must be valid, even assuming a tiny number of edits, the search space of valid edits can be vast indeed.

As specified, this particular game has a few other rules that we self-impose: the solver must seek out plausible repair(s) first, but eventually discover every syntactically valid repair within a certain number of edits from the original invalid source code. Furthermore, the solver must sample repairs without replacement in parallel across all available cores and whensoever interrupted, return the best candidates hitherto discovered. Finally, it must be able to repair programs in a variety of languages without any prior statistical knowledge. These constraints introduce a number of interesting design challenges, which will be addressed in the following section.

\section{Approach}

%How did you go about solving or making progress on the problem? What new understanding, knowledge, methods, or technologies will this research generate?

During the conception of Tidyparse, a number of design choices were made to increase its utility as a real-time programming assistant, which aims to provide precise and continuous feedback while the user is typing across a variety of programming languages. To support this use case, the following criteria were taken into account when designing and evaluating various components of the repair procedure.

\begin{itemize}
  \item First and foremost, the framework must be \textbf{sound} and \textbf{complete}. That is, the parser-generator must (1) accept arbitrary context-free grammars, and (2) generate a parser which accepts all and only syntactically valid strings and (3) when a string is invalid, our synthesizer must eventually generate every syntactically valid string within a fixed distance from it.
  \item Second, we require that the resulting repairs be \textbf{plausible} and \textbf{diverse}. In other words, the framework should generate repairs that are likely to be written by a human being, consistent with the surrounding context, and reasonably diverse in their structure.
  \item Third, the framework must be \textbf{efficient} and \textbf{responsive}. It must be able to recognize well-formed strings in subcubic time, and generate admissible repairs in subpolynomial time. These conditions are necessary to provide real-time feedback whilst the user is typing.
  \item Fourth, the framework must also be \textbf{robust} and \textbf{scalable}. In practice, this means that the framework should be robust to multiple errors, handle grammars with a large number of productions and be able to scale linearly as the number of processor cores are increased.
  \item Fifth, the framework must be \textbf{flexible} and \textbf{extensible}. As a general-purpose tool for generating syntax repairs in a wide variety of programming languages, end-users should be able to extend the framework with their own custom grammars and side-constraints.
\end{itemize}

As we demonstrate both formally and experimentally, our framework is able to satisfy most of these criteria simultaneously, while attaining state-of-the-art performance in terms of precision, throughput and wall-clock latency when compared with a variety of neural and symbolic baselines.

We can reframe syntax error correction as a language edit distance problem, but instead of using the typical approach that only returns the nearest repair(s) by edit distance, we try to find every single repair within a fixed distance. This problem can be modeled as a conjunctive language membership query between a context-free grammar recognizing the syntax and an automaton that recognizes the Levenshtein ball around an invalid string. At a high level, we take the intersection of the respective grammars' languages, encode them as a single conjunctive language, extract its members, then rank the repairs by a suitable distance metric such as Levenshtein or finger-travel distance on a keyboard.

Crucially, repairs must be generated quickly, as the usefulness of syntax corrections are inversely proportional to the time it takes to generate them. To enable low-latency repairs, we develop a novel reduction from conjunctive language reachability onto finite field arithmetic, based on the idea of matrix multiplication with a logical semiring. As originally shown by Valiant~\cite{valiant1975general}, context-free language recognition can be reduced to iterated Boolean matrix multiplication. When the string contains holes, we can abstractly interpret Valiant's algorithm (our work) to solve for the admissible completions, representing syntax repairs. This reduction is highly efficient from a complexity-theoretic standpoint, letting us solve for the missing values using a SAT solver, and the resulting valid solutions can be reranked by a distance metric, then presented to the user for manual review.

We have implemented our approach as an IDE plugin and demonstrated its viability as a practical tool for realtime programming. A considerable amount of effort was devoted to supporting fast error correction functionality. Tidyparse is capable of generating rapid and natural repairs for invalid code in a range of practical programming languages using only the grammar and a cheap ranking metric over strings.

%While efficient, the above procedure produces many syntactically valid but unnatural edits. To reduce the throughput under realtime constraints, we can solve for the locations using a stochastic local search procedure that initially samples from the Levenshtein ball, then as repairs are discovered and ranked, we can use the resulting distance metric to guide the search towards more promising regions of the search space.

\section{Evaluation Methodology}

%In writing the evaluation methodology section of your submission, we encourage you to emphasize two main aspects of your experiment:

There are essentially two ways to evaluate a syntax error corrector: (1) measure the rate at which repairs are accepted by the true parser, or (2) measure the rate at which human repairs are matched. The former approach is technically correct, but does not distinguish between correct and natural repairs. Since most parsers are relatively permissive, (1) tends to admit a variety of repairs that are syntactically valid but otherwise unnatural or practically useless in the absence of further semantic constraints or a strong ranking metric.

The latter evaluation considers both human preferences and syntactic validity. While human repairs are generally more challenging to discover, next to genuine user telemetry, they are arguably the best surrogate for human preference. Although pragmatism generally favors (2), pairwise error-fix datasets can be unavailable in minority languages and can be misleading when considering out-of-distribution errors. We therefor consider (1) for evaluating throughput, soundness and completeness of the solver, and (2) to measure its precision and latency under realistic usage conditions.

We evaluate Tidyparse on an extensive suite of syntax errors in Python code, comparing it with Seq2Parse~\cite{sakkas2022seq2parse} and Break-It-Fix-It (BIFI)~\cite{yasunaga2021break}, two transformer-based syntax correction models. We compare our Precision@\{1, 5, 10, All\} with repairs of varying difficulty, from single- to multi-token insertions, deletions and substitutions, and measure throughput and wall-clock latency across a wide variety of error categories and timeout values. We also evaluate Tidyparse on synthetic errors in synthetic languages and synthetic errors in natural programming languages to evaluate our robustness on rare grammars and error categories.

\subsection{Hypothesis}

%What would be the main research result? What would be the secondary research results? Phrase these as primary and secondary hypotheses.

By carefully leveraging parallelism, fast parsing and a cheap ranking function, we hypothesize it is (1) possible to achieve a higher overall Precision@\{5+\} at a lower latency than the existing transformer-based repair models for repairs up to three tokens, and furthermore can do so without using any neural network whatsoever. Additionally, we hypothesize it is (2) possible to retrain a smaller, faster transformer-based model to score and rerank the repairs discovered by Tidyparse, and this hybrid model can be used to attain SoTA precision, throughput and wall-clock latency across all relevant categories, validating the merits of pragmatism.

\subsection{Evaluation Setup}

%How are you going to set up your experiments or studies to test these hypotheses? How do you plan to control for bias? How will you maximize external validity?

Using a pairwise dataset of human syntax errors and repairs from Python snippets on StackOverflow~\cite{wong2019syntax}, we preprocess the data to include only syntax pairs whose broken form is rejected by the true parser, and whose repair is accepted by the true parser. This dataset is then abstracted using a lexical tokenization, deduplicated, and all pairs whose Levenshtein distance is greater than three lexical edits are removed.

For our synthetic evaluation, we synthetically corrupt parseable single-line statements in Java and Kotlin source code by sampling edits uniformly at random from the Levenshtein ball and rejecting any edits accepted by the true parser. Treating the original source code as the ground-truth repair, we then measure the average latency to recover the original statement, and the precision at varying latency cutoffs.

External validity is maximized by evaluating on a large dataset of human syntax errors and repairs across a variety of programming languages and grammars. We qualitatively control for bias by using an identical test set for all models and balancing across edit distances and error categories to model a diversity of real-world error-correction scenarios. Although pairwise repair data is limited due to the paucity of erroneous source code committed to public GitHub repositories and reported on StackOverflow, we can ensure the test set is largely representative of real world syntax errors. In the future, we hope to conduct a proper user study.

%bibliography:
\bibliography{doctoral_symposium}
\bibliographystyle{ACM-Reference-Format}
\end{document}
\endinput
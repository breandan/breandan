\pdfoutput=1
%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,10pt,review,anonymous]{acmart}
%\settopmatter{printfolios=false,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
\documentclass[sigplan,nonacm]{acmart}\settopmatter{printfolios=false,printccs=false,printacmref=false}

%% Bibliography style
\bibliographystyle{acmart}

\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\begin{document}
  \title{Survey on Constrained LLM Decoding}
  \begin{abstract}
    Recent advances in language model (LLM) decoding have led to the publication of a lot of papers that claims to always be syntactically valid. However, the reliability and efficiency of these methods vary widely in practice. We survey the landscape of language model decoding methods, focusing on those which claim to generate only valid programs. We compare the soundness, completeness, naturalness, parallelism and tooling of each approach.
  \end{abstract}

  \author{Breandan Considine}
  \affiliation{\institution{McGill University}}
  \email{bre@ndan.co}

  \author{Xujie Si}
  \affiliation{\institution{University of Toronto}}
  \email{six@cs.utoronto.ca}

  \maketitle


  \section{Introduction}

  Many methods to sample from LLMs have been proposed. Some of these guarantee that all samples are grammatically valid. Others guarantee that all grammatically valid samples are generable. Still others guarantee that all samples are unique, i.e., converge linearly to the language.

  The trick is not just synthesizing valid functions, but doing so in a parallel communication-free manner, without compromising soundness or completeness. You want to massively scale up a discrete sampler without replacement. One very efficient way of doing this is by constructing an explicit bijection from the sample space to the integers, sampling integers, then decoding them into programs.While this technique enjoys certain advantages, i.e., it is embarrassingly parallelizable and guaranteed to enumerate distinct solutions with a bounded delay, it also somewhat unnatural. By flattening the distribution onto the integers a la Gödel numbering, it destroys locality, does not play well with incremental decoding methods (left-to-right is currently en vogue in generative language modeling), and will fail if the sample space is uncountable.

  Maciej Bendkowski has some related work [1] on generating random lambda terms, but was unable to overcome what he calls the asymptotic sparsity problem:

  ``Sampling simply-typed terms seems notoriously more challenging than sampling closed ones. Even rejection sampling, whenever applicable, admits serious limitations due to the imminent asymptotic sparsity problem — asymptotically almost no term, be it either plain or closed, is at the same time (simply) typeable.''

  Victor Maia has done some interesting work on this topic, but it is not clear how scalable it is. Another key step is reducing symmetries in your sample space by quotienting it somehow (e.g., by $\alpha$-equivalence). The author seems to be invoking some kind of equivalence relation by ``superposition'', but the technical details here are a little fuzzy.

  This problem is also closely related to model counting in the CSP literature, so a practical speedup could lead to improvements on a lot of interesting downstream benchmarks.

  In general, the problem of program induction from input-output examples is not well-posed, so specialized solvers that can make stronger assumptions will usually have an advantage on domain-specific benchmarks. Most existing program synthesizers do not satisfy all of these desiderata, e.g., soundness (\textbf{S}), completeness (\textbf{S}), naturalness (\textbf{N}), and parallelism (||). It depends on how you define ||, but basically, we want to decode in parallel. So an LLM that uses a GPU we do not consider to be ``parallel'' in the sense we mean here.

  \newcommand{\tidyparse}{\href{https://arxiv.org/pdf/2408.01849}{Tidyparse}~\cite{considine2023pragmatic}}
  \newcommand{\seqtoparse}{\href{https://pg.ucsd.edu/publications/Seq2Parse-neurosymbolic-parse-error-repair_OOPSLA-2022.pdf}{Seq2Parse}~\cite{sakkas2022seq2parse}}
  \newcommand{\bifi}{\href{https://arxiv.org/pdf/2106.06600}{BIFI}~\cite{yasunaga2021break}}
  \newcommand{\ordinalfix}{\href{https://arxiv.org/pdf/2309.06771}{OrdinalFix}~\cite{zhang2023ordinalfix}}
  % https://boxbase.org/entries/2019/nov/25/error-correcting-earley-algorithm/
  \newcommand{\ahopeterson}{\href{https://epubs.siam.org/doi/10.1137/0201022}{Aho/Peterson}~\cite{aho1972minimum}}

  \begin{table}[h]
    \begin{tabular}{c|cccccc}
                   & \textbf{S}       & \textbf{C} & \textbf{N} & \textbf{Theory} & ||     & \textbf{Tool} \\\hline
      \tidyparse   & \cmark           & \cmark & \cmark & CFG$_\cap$      & \cmark & IDE-ready     \\
      \seqtoparse  & \cmark$^\dagger$ & \xmark & \cmark & CFG             & \xmark & Python        \\
      \bifi        & \xmark           & \xmark & \cmark & $\Sigma^*$      & \xmark & Python        \\
      \ordinalfix  & \cmark           & \xmark & \xmark & CFG+            & \xmark & Rust          \\
      \ahopeterson & \cmark           & \xmark & \xmark & CFG             & \xmark & None          \\
    \end{tabular}
    \end{table}

  Now, we consider just LLM-based SyGuS synthesizers.

  \newcommand{\outlines}{\href{https://arxiv.org/pdf/2307.09702}{Outlines}~\cite{willard2023efficient}}
  \newcommand{\syncode}{\href{https://arxiv.org/pdf/2403.01632}{SynCode}~\cite{ugare2024improving}}
  \newcommand{\gad}{\href{https://arxiv.org/pdf/2405.21047}{GAD}~\cite{park2024grammar}}
  \newcommand{\codeguard}{\href{https://arxiv.org/pdf/2405.00218}{CodeGuard+}~\cite{fu2024constrained}}
  \newcommand{\flap}{\href{https://arxiv.org/pdf/2403.05766}{FLAP}~\cite{roy2024flap}}
  \newcommand{\domino}{\href{https://arxiv.org/pdf/2403.06988}{DOMINO}~\cite{beurer2024guiding}}

  \begin{table}[h]
    \begin{tabular}{c|cccccc}
                                           & \textbf{S}       & \textbf{C}       & \textbf{N} & \textbf{Theory} & ||     & \textbf{Tool} \\\hline
      \outlines  & \cmark$^\dagger$ & \cmark$^\dagger$ & \cmark     & CFG             & \xmark & Python        \\
      \syncode   & \cmark           & ?                & \cmark     & CFG             & \xmark & Python        \\
      \gad       & \cmark           & ?                & \cmark     & CFG             & \xmark & Python        \\
      \codeguard & \cmark           & ?                & \cmark     & CFG             & \xmark & Python        \\
      \flap      & \cmark           & ?                & \cmark     & CFG             & \xmark & Python        \\
      \domino    & \cmark           & \xmark           & \cmark     & CFG             & \xmark & Python
    \end{tabular}
  \end{table}


  Also, we consider discrete program search and enumerative search techniques that do not use an LLM, but allow some semantic constraints on the generated program.

  \newcommand{\boltzmann}{\href{https://arxiv.org/pdf/2206.06668}{Boltzmann Brain}~\cite{bendkowski2022automatic}}
  \newcommand{\dps}{\href{https://arxiv.org/pdf/2403.06988}{IntCalc}~\cite{beurer2024guiding}}

   \begin{table}[h]
    \begin{tabular}{c|cccccc}
                                                     & \textbf{S} & \textbf{C} & \textbf{N} & \textbf{Theory} & ||     & \textbf{Tool} \\\hline
      Boltzmann Brain~\cite{bendkowski2022automatic} & \cmark     & \cmark     & \cmark     & UT$\lambda$C    & \cmark & Python        \\
      OrdinalFix~\cite{zhang2023ordinalfix}          & \cmark     & ?          & \xmark     & N/A             & \cmark & Rust          \\
      Bend/DPS                                       & \cmark     & ?          & ?          & IntCalc         & \cmark & CUDA
    \end{tabular}
  \end{table}

  Another important feature is incrementality, which none of the existing synthesizers really implement properly. Basically, you want the ability to compute quotients, i.e.:

  \begin{equation}
    L_{1}/L_{2}=\{w\in \Sigma ^{*}\mid \exists x\in L_{2}\colon \ wx\in L_{1}\}
  \end{equation}

  \begin{equation}
    L_{2}\backslash L_{1}=\{w\in \Sigma ^{*}\mid \exists x\in L_{2}\colon \ xw\in L_{1}\}
  \end{equation}

  \clearpage

  \bibliography{bib}
\end{document}
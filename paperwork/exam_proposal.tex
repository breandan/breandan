%! Author = breandan
%! Date = 11/16/20

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage[pdf]{graphviz}
\usepackage{amssymb}
\usepackage{amsfonts}
%\usepackage{prooftrees}
\usepackage{bussproofs}
\usepackage{hyperref}
\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}
\usepackage{natbib}
\usepackage{float}
\usepackage{xcolor}


\title{Pattern Recognition in Procedural Knowledge}
\author{Breandan Considine}
\date{\today}

% Document
\begin{document}
    \maketitle

    \tableofcontents
    \pagebreak


    \section{Introduction}

    Historically, most knowledge was stored as natural language. A growing portion is now \textit{code}~\citep{allamanis2018survey}. Code is procedural knowledge intended for execution by a machine. Though it shares many statistical properties in common with natural language~\citep{hindle2012naturalness}, code is written in a formal language with a deterministic grammar and denotational semantics~\citep{pierce2010software}. We can use this specification to precisely reason about operational or procedural correctness.

    Prior work explored differentiable programming~\citep{considine2019programming}. Differentiability plays a key role in learning, but does not provide the necessary vocabulary to describe human knowledge. In order to capture human knowledge and begin to reason as humans do, programs must be able to express the concept of \textit{uncertainty}. In this work, we propose a set of tools and techniques for reasoning about uncertainty in the form of procedural knowledge.

    To reason about procedural knowledge, we must first define what it means for two procedures to be equal. Although equality is known to be undecidable in most languages, various equivalence tests and semi-decision procedures have been developed. For example, we could rewrite said procedures~\citep{baader1999term}, compare them in various contexts~\citep{felleisen1990expressive}, and simulate or execute them on various input data~\citep{chen2020metamorphic} so as to ascertain their exact relationship.

    In practice, exact equality is too rigid to operationalize. A more useful theory would allow us to compare two procedures in the presence of naturally-arising stochasticity. What is the probability of observing local variations? How are those observations related? And how do local variations affect global behavior? In order to correctly pose these questions and begin to answer them, we must be able to reason probabilistically.

    Graphs are a natural representation for both procedural knowledge~\citep{allamanis2017learning} and probabilistic reasoning~\citep{pearl2014probabilistic}. The language of linear algebra provides a unifying framework for many graph algorithms and program analysis tasks~\citep{kepner2011graph}. Recent evidence suggests probabilistic inference is tractable for a large class of graphical models~\citep{choi2020probabilistic}. And sparse matrix representations enable efficient processing of large graphs on modern graphics processors~\citep{kepner2016mathematical}.

    In this work, we will define exact and approximate equality and cover some deterministic and probabilistic algorithms for deciding it. We will then describe a few graph representations for encoding approximate procedural knowledge. Finally, we will discuss some opportunities for applying these ideas to search-based software engineering, in particular code search, vulnerability detection, fault localization and program repair.


    \section{From exact to approximate equality}\label{sec:definitions}

    Equality is a highly overloaded notation in mathematics and computer science. Loosely speaking, $x = y$ denotes when we can replace, or are replacing one expression $x$, with another, $y$. Typically, we consider two expressions to be equal if there is no deterministic procedure which can tell them apart.

    The trouble is, equality is not a differentiable operator. If two expressions are unequal, the derivative alone cannot tell us how to adjust one in order to bring them closer together. Consider the Kronecker $\delta$-function, $\delta_k: \mathbb{N}^2\rightarrow \mathbb{B}$:

    $$
    \delta_k(x, y) :=
    \begin{cases}
        1 \text{ if } x = y, \\
        0 \text{ otherwise }\\
    \end{cases}
    $$

    When we encounter a differentiable program containing $\delta_k$, how do we compute its derivative? Since $\mathbb{B}$ is not open, $\delta_d^{-1}(B\subset \mathbb{B})$ is not open, thus $\delta_d$ is not a continuous function and its gradient $\nabla\delta_k$ is undefined. Now consider the Dirac $\delta$ function, $\delta_d: \mathbb{R}^2 \rightarrow \mathbb{R}$, a probability distribution on $\mathbb{R}$:

    $$
    \iint_{-\infty}^{\infty} \delta_d d(x, y) := 1 \text{ and } \delta_d(x, y) :=
    \begin{cases}
        \infty \text{ if } x = y, \\
        0 \text{ otherwise }\\
    \end{cases}
    $$

    Unlike $\nabla\delta_k$, it can be shown that $\nabla\delta_d$ is well-defined everywhere on $\mathbb{R}^2$. Here we encounter an important distinction between intensional and extensional equality. Unlike symbolic expressions, humans have invented many useful functions which can only be described indirectly but cannot be constructed explicitly, e.g. a probability distribution with mass concentrated in an infinitesimally small region. Nevertheless, they represent convenient abstractions for modeling many physical and computational processes.

    Neither $\nabla\delta_k$ nor $\delta_d$ are possible to implement on a computer without loss of generality. How does one compute $\nabla\delta_k$ without knowing $x$ or $y$ a priori? What value of $\infty$ is sufficient to define $\delta_d$? To allow a more flexible definition of the $=$ operator, we require a function which approximates the logical properties of $\delta_k$, but which can be made differentiable like $\delta_d$. A more general notion is the concept of an \textit{equivalence relation}. An equivalence relation $\sim$ is a binary relation with the following logical properties:

    \begin{prooftree}
        \bottomAlignProof
        \AxiomC{}
        \UnaryInfC{$a \sim a$}
        \noLine
        \UnaryInfC{}
        \noLine
        \UnaryInfC{\textit{Reflexivity}}
        \DisplayProof
        \hskip 1.5em
        \bottomAlignProof
        \AxiomC{$a \sim b$}
        \UnaryInfC{$b \sim a$}
        \noLine
        \UnaryInfC{}
        \noLine
        \UnaryInfC{\textit{Symmetry}}
        \DisplayProof
        \hskip 1.5em
        \bottomAlignProof
        \AxiomC{$a \sim b$}
        \AxiomC{$b \sim c$}
        \BinaryInfC{$a \sim c$}
        \noLine
        \UnaryInfC{}
        \noLine
        \UnaryInfC{\textit{Transitivity}}
        \DisplayProof
        \hskip 1.5em
        \bottomAlignProof
        \AxiomC{$a \sim b$}
        \UnaryInfC{$f(a) \sim f(b)$}
        \noLine
        \UnaryInfC{}
        \noLine
        \UnaryInfC{\textit{Congruence}}
    \end{prooftree}

%    First we will describe exact equality, then progressively relax our definition of equivalence to obtain a more useful definition.

%    We require a metric~\ref{sec:probabilistic}.

    \subsection{Decidability}\label{sec:algorithms}

    To determine whether two expressions are equal, we need a decision procedure. Various approaches have been taken to decide exact and approximate equality in the deterministic and probabilistic setting. We list a few below.

    \begin{table}[H]
        \centering
        \begin{tabular}{c|l|l|}
            \cline{2-3} & \textbf{Deterministic} & \textbf{Probabilistic} \\ \hline
            \multicolumn{1}{|c|}{\textbf{Exact}} & \begin{tabular}[c]{@{}l@{}}
                                                       Type Checking\\ Model Checking
            \end{tabular} & \begin{tabular}[c]{@{}l@{}}
                                Variable Elimination\\Probabilistic Circuits
            \end{tabular} \\ \hline
            \multicolumn{1}{|c|}{\textbf{Approximate}} & \begin{tabular}[c]{@{}l@{}}
                                                             Software Testing\\Dynamic Analysis
            \end{tabular} & \begin{tabular}[c]{@{}l@{}}
                                Monte Carlo Methods\\Bayesian Networks
            \end{tabular} \\ \hline
        \end{tabular}
    \end{table}

    It is seldom the case that two semantically equal expressions are trivially equal: we must first perform some computation to establish their equality. In the exact setting, this procedure might be summarized as follows:

    \begin{enumerate}
        \item Rewrite: Either transform the proposition into normal form if one exists, or enumerate valid rewrites using an enumerative search, then
        \item Compare: Perform some trivial (e.g. $\mathcal{O}(n)$) comparison.
    \end{enumerate}

    Unfortunately, exact equality is known to be undecidable in general (G\"odel). From Turing, we know there is no way to construct a halting machine which accepts every equality and rejects every disequality in a universal language. Equality between two mathematical formulas is undecidable. (Richardson) Even equality in first-order logic is undecidable. Furthermore any nontrivial property is undecidable. (Rice)

    %Is there any left hope? Yes! (Felleisen)

    Tractability may be related to, but is not contingent upon decidability. When decidable, equality may be intractable in practice, and languages where equality is undecidable may have decidable fragments. But even when exact equality is intractable, we may be able to construct a probabilistic decision algorithm, or semidecision procedure which terminates for all or most practical purposes. The latter approaches fall into two broad categories:

    \begin{itemize}
        \item Execute: Evaluate the program by running it on a small set of inputs
        \item Sample: Build a probabilistic model and sample from its distribution
    \end{itemize}

    First we will describe our notion of intensional and extensional equality, then progressively relax our definition to consider probabilistic equivalence.


%    Mathematics is often a useful approximation of reality, but many mathematical concepts are impossible to realize. This is due to two problems:
%
%    \begin{enumerate}
%        \item intrinsic: the mechanics of the system are intrinsically unsound or impossible to mechanize. How do we encode $\lim_{x \rightarrow \infty}$?
%        \item extrinsic: all descriptions (epistemic) and observations (alleoteric) are approximate and this error can compound quickly.
%    \end{enumerate}


%    But can we organize mathematical knowledge as a decision tree? (Rich)

%    Still, we can answer many questions exactly. If we restrict the reasoning system to incomplete queries, we can be consistent.

    \subsection{Intensional equivalence}\label{subsec:intensional-equivalence}

    Let $\Sigma: (\mathcal{I} \rightarrow \mathcal{R}) \times (\mathcal{I}\rightarrow \mathcal{R})$ be a binary relation on functions which are closed under composition. We say that two functions $f, f' \in \mathcal{I} \rightarrow \mathcal{R}$ are intensionally equal or disequal under $\Sigma$ if we can establish $(f,f') \in \Sigma^*$ via:

    \begin{prooftree}
        \AxiomC{$f: \mathcal{I} \rightarrow \mathcal{R}$}
        \RightLabel{\textsc{Init}}
        \UnaryInfC{$\Gamma^0_{f} \vdash \{f\}, \{(f, f)\}$}
%        \UnaryInfC{\textit{Commutivity}}
        \DisplayProof
        \hskip 1em
        \AxiomC{$\Gamma^{n}_{f} \vdash E \subseteq (\mathcal{I} \rightarrow \mathcal{R})^n, G \subseteq E \times E$}
        \RightLabel{\textsc{Sub}}
        \UnaryInfC{$\Gamma^{n+1}_{f} \vdash \bigcup\limits_{\substack{e \in E\\\sigma \in \Sigma}}e'\leftarrow e[\sigma_1\rightarrow\sigma_2], (e, e')$}
        \DisplayProof
        \vskip 1em
        \AxiomC{$\Gamma^n_{f} \vdash E, G$}
        \AxiomC{$f' \in E$}
        \RightLabel{\textsc{Eq}}
        \BinaryInfC{$\Gamma^n_{f} \vdash f = f'$ by $G^{-n}(f')$}
        \DisplayProof
        \hskip 1em
        \AxiomC{$\Gamma^n_{f}\vdash E$}
        \AxiomC{$\Gamma^{n+1}_{f}\vdash E$}
        \AxiomC{$f'\notin E$}
        \RightLabel{\textsc{Neq}}
        \TrinaryInfC{$\Gamma^{n+1}_{f} \vdash f \neq f'$}
    \end{prooftree}

    \noindent For example, suppose we are given $f := a b c, f' := c b a$ and the rewrite system $\Sigma := \{(a, a), (ab, ba)\}$. Indeed, we can establish $\textsc{Eq}[f, f']$ as follows:

    \begin{prooftree}
        \def\fCenter{\ :=\ }
        \Axiom$f \fCenter a b c$
        \def\fCenter{\ \vdash\ }
        \RightLabel{\textsc{Init}}
        \UnaryInf$\Gamma^0_{f} \fCenter \{abc\}, \{(abc, abc)\}$
        \RightLabel{\textsc{Sub}}
        \UnaryInf$\Gamma^1_{f} \fCenter \{\ldots, bac, acb\}, \{\ldots, (abc, bac), (abc, acb)\}$
        \RightLabel{\textsc{Sub}}
        \UnaryInf$\Gamma^2_{f} \fCenter \{\ldots, bca, cab\}, \{\ldots, (bac, bac), (acb, acb), (bac, bca), (acb, cab)\}$
        \RightLabel{\textsc{Sub}}
        \UnaryInf$\Gamma^3_{f} \fCenter \{\ldots, \mathbf{cba}\}, \{\ldots, (bca, bca), (cab, cab), (cab,\textbf{cba})\}$
%        \RightLabel{Eq}
%        \UnarbInf$\Gamma^3_{f} \fCenter E \hskip 3em f' \vdash cba \in E$
        \RightLabel{\textsc{Eq}}
        \UnaryInf$\Gamma^3_{f} \fCenter f=f'\text{ bb } G^{-3}(f':=cba) = \{f := abc\}$
    \end{prooftree}

    \noindent We can visualize $G$ as a directed graph, omitting all loops. Notice how each path converges to the same term, a property known as $\textit{strong confluence}$.

    \hspace{12pt}\digraph[scale=0.5, center=true]{abc}{
    rankdir=LR;
    len=3;
    node [shape=Mrecord];

    a [ label="abc"; ]
    b [ label="bac"; ]
    c [ label="acb"; ]
    d [ label="bca"; ]
    e [ label="cab"; ]
    f [ label="cba"; ]

%    a -> a [label="Σ₀"]
    a -> b [label="Σ₁" minlen=2]
    a -> c [label="Σ₁" minlen=2]
    b -> d [label="Σ₁" minlen=2]
    c -> e [label="Σ₁" minlen=2]
    e -> f [label="Σ₁" minlen=2]
    d -> f [label="Σ₁" minlen=2]
    }

    \noindent Let $|\mathcal{I}|, |\Sigma^*| \in \mathbb{N}$ and consider the complexity of $\textsc{Neq}[g, g'], \forall g\neq g' \in \mathcal{I} \rightarrow \mathcal{R}$. Supposing $\mathcal{O}_{\textsc{Sub}}(1)$, it can be shown the above procedure will terminate in

    $$\underset{|\mathcal{I}|}{\text{min}}\{n \mid \Gamma_g^n = \Gamma_g^{n+1}\} = \Theta(|\mathcal{I}|) \text{ time, } \underset{i \in [0, n]}{\text{max}}\{|E| \mid \Gamma_g^i \vdash E\} = \Theta(|\mathcal{I}|!) \text{ space}$$

    \noindent Termination is not always guaranteed, e.g. $\Sigma' := \Sigma \cup \{(a, 1a)\}$. While trivial to show for $\Sigma'$, termination under arbitrary $\Sigma$ is known to be undecidable.

    \subsection{Extensional equivalence}\label{subsec:extensional-equivalence}

    As we have seen, the procedure described in Section ~\ref{subsec:intensional-equivalence} is limited by $|\mathcal{I}|$. For intensional equivalence $\mathcal{O}_{\textsc{Eq}} = \Theta_{\textsc{Neq}}$ although $\mathbb{E}_{g=g'}[\Theta_\textsc{Eq}]$ is tractable. For extensional equivalence we will see the opposite holds, ceteris paribus.

    \begin{prooftree}
        \AxiomC{$\Gamma \vdash f: \mathcal{I} \rightarrow \mathcal{R}$}
        \AxiomC{$\Gamma \vdash r, r' \in \mathcal{R}$}
        \AxiomC{$|\mathcal{R}| \in \mathbb{N}$}
        \RightLabel{\textsc{Call}}
        \BinaryInfC{$\Gamma \vdash f(i): \mathcal{R}$}
%        \UnaryInfC{\textit{Commutivity}}
        \DisplayProof
        \hskip 1em
        \RightLabel{\textsc{Sub}}
        \AxiomC{$\Gamma \vdash f: \mathcal{I} \rightarrow \mathcal{R}$}
        \AxiomC{$f(i)\leadsto r$}
%        \AxiomC{$i: \mathcal{I}$}
        \BinaryInfC{$\Gamma \vdash f'\leftarrow f[e(i)\rightarrow r], (e, e')$}
        \DisplayProof
        \vskip 1em
        \AxiomC{$\Gamma \vdash \forall i \in \mathcal{I}, \textsc{Eq}(f(i), f'(i)) $}
        \RightLabel{\textsc{Eq}}
        \UnaryInfC{$\Gamma \vdash f = f'$}
        \DisplayProof
        \hskip 1em
        \AxiomC{$\Gamma \vdash \exists i \in \mathcal{I}, \textsc{Neq}(f(i), f'(i)) $}
        \RightLabel{\textsc{Neq}}
        \UnaryInfC{$\Gamma \vdash f \neq f'$}
    \end{prooftree}

    Suppose $\mathcal{R}$ has a normal form such that $\mathcal{O}(r_1\overset{?}{=}r_2) = \mathcal{O}(1), \forall r_1, r_2 \in \mathcal{R}$.

    ``Do $f_1$ and $f_2$ behave in the same way over all inputs?" While extensional equality is difficult to show if $|\mathcal{I}|$ is large, detecting inequality for pairs of random functions is relatively easy: we can simply search for $\hat i \in \mathcal{I}$ such that $f_1(\hat i) \neq f_2(\hat i)$.

    % big step semantics

    For example, suppose we have two functions $f_1, f_2: \mathbb{Z}^3 \rightarrow \mathbb{Z}$ where $f_1(x)=-x^3$ and $f_2(x)=|x|^3$. If we test $\hat i \in {-2, -1, 0, 1, \ldots}$, we have $f_1(-2)=f_2(-2)$, $f_1(-1)=f_2(-1)$, $f_1(0)=f_2(0)$, $f_1(1) \neq f_2(1)$. Once we detect an $f_1(\hat i) \neq f_2(\hat i)$, we can halt immediately.

    We can test using property-based / metamorphic testing. Some strategies:

    Isomorphism testing

    Metamorphic testing

    \subsection{Contextual Equivalence}

%    http://www.cs.ox.ac.uk/people/samuel.staton/papers/fossacs-2019.pdf
%    http://users.ox.ac.uk/~scro3229/documents/birmingham-talk.pdf

    Two terms $\textt{M}$ and $\textt{N}$ are contextually equivalent iff $\forall \textt{C[...]}$ where $\textt{C[M]}$ is valid in our language and halts, $\textt{C[N]}$ is also valid and halts. This is kind of like word embeddings.

    $P(w_t = a | w_{t-1}, w_{t-2}\ldots, w_{t+1}, w_{t+2}, \ldots)\overset{?}{\approx} P(w_t = b | w_{t-1}, w_{t-2}\ldots, w_{t+1}, w_{t+2}, \ldots)$

    We can define semantic equality in our setting using word embeddings.

%    Mathematics seems to be obsessed with completeness, much to its detriment. Completeness invades every definition. Definitions must hold over all cases, including the adversarial ones. This is good, because it encourages rigor in a highly adversarial field. But we must not confuse rigor with truth.
%
%    The truth is, most people prefer useful abstractions to complete specifications. Most people just want to write dependable software.
%
%    To be more inclusive of our audience, we should optimize our definitions for consistency, at the expense of completeness.

    \subsection{Metrics}\label{sec:probabilistic}

    How do we know when we are approaching equality? We can use a distance metric.

    % https://towardsdatascience.com/beyond-weisfeiler-lehman-approximate-isomorphisms-and-metric-embeddings-f7b816b75751

    Various graph distance metrics have been proposed~\citet{sanfeliu1983distance}.

    Hypothesis testing

    Kolmogorov-Smirnov

    Kantorovich-Rubinstein

    L\'evy-Prokhorov

    Gromov-Hausdorff

    Weisfeiler-Lehman

    TODO: Define probability distributions, integration, kernel functions and metrics.

    We want a metric $M_\theta: (\mathcal{I}\rightarrow{O}) \times (\mathcal{I}\rightarrow{O})\rightarrow \mathbb{R}$ between two functions, which predicts their semantic similarity. In other words, the closer two functions are with respect to $M_\theta$, the more likely they are to be equal. During test time, we query a dataset for the $k$ most similar functions, and try to unify them extensionally and intensionally. We can train a metric $M_\theta$ and discriminator $D_\theta$ on pairs of random functions $f_1$ and $f_2$, to predict their similarity. During inference, we let the discriminator sample random inputs $\hat i_1 \ldots n \sim D_\theta(\hat i \mid f_1, f_2)$ from its latent distribution, conditioning on the structure of $f_1$ and $f_2$. In other words, we want a model that predicts similarity and outputs values which are likely to demonstrate instances of inequality.

    \pagebreak


    \section{From computation to knowledge graphs}\label{sec:graphs}

    % https://en.wikipedia.org/wiki/Duality_(optimization)

    Duality is an important concept in mathematical optimization. Many optimization problems can be seen as dual to each other: KKT and SVM duality. Duality occurs in automatic differentiation with dual number arithmetic.

    Duality is also an important concept in computer science. One famous example is the duality between code and data: in \textit{homoiconic} languages, we can treat code as data and data as code. cf. Kleene's recursion theorem.

    One way to view automatic differentiation is that it allows us to compute the sensitivity of numerical values in a fixed computation graph. What we wanted to compute sensitivities with respect to changes in the computation graph itself?

    \subsection{Algebraic graphs}\label{subsec:algebraic-graphs}

    Graphs can be modeled algebraically~\citep{weisfeiler1968reduction}  using algebraic data types \citep{mokhov2017algebraic}.

    Graphs are algebraic structures...

    Semirings arise in strange and marvelous places. (min, +), (max, \times)

    \begin{enumerate}
        \item https://people.cs.kuleuven.be/~luc.deraedt/Francqui4ab.pdf#page=71
        \item http://www.mit.edu/~kepner/GraphBLAS/GraphBLAS-Math-release.pdf#page=11
    \end{enumerate}

    \subsection{Programs are graphs}\label{sec:program-graphs}

    computation graphs~\citep{breuleux2017automatic} in machine learning

    e-Graphs~\citep{willsey2020egg} in reasoning systems

    arithmetic circuits~\citep{miller1988efficient} in numerical computing

    probabilistic circuits~\citep{choi2020probabilistic} in probabilistic modeling community

    \subsection{Probabilistic graphical models}\label{sec:pgms}

    Probabilistic graphical models (PGMs) are very expressive, but even approximate inference on belief networks (BNs) is NP-hard~\citep{dagum1993approximating} We can faithfully represent a large class of PGMs and their corresponding distributions as probabilistic circuits (PCs)~\citep{choi2020probabilistic}, which are capable of exact inference in polynomial time and empirically tractable to calibrate using SGD or EM. PCs share many algebraic properties with PGMs and can propagate statistical estimators like variance and higher moments using simple rules.

    \subsection{Knowledge graphs}

    Knowledge graphs~\citep{hogan2020knowledge} are multi-relational graphs whose nodes and edges possess a type. Two entities can be related by multiple types, and each type can relate many pairs of entities. We can index an entity based on its type for knowledge retrieval, and use types to reason about compound queries, e.g. ``Which company has a direct flight from a port city to a capital city?''

%    \subsection{Message passing algorithms}
%
%    Many algorithms can be implemented as message passing on graphs...

    \pagebreak


    \section{From procedural knowledge to code}\label{sec:applications}

    Experts typically encode their knowledge using pen and paper and leave the rest of us mere mortals to decipher it. It is somewhat tedious, but generally possible for skilled programmers to translate textual information into computer programs. Unfortunately, there are many equivalent ways to translate text into code -- the same algorithm implemented in the same language by different authors is seldom written the same way. Usually we end up reinventing the wheel. So we need some mechanism to detect exact or approximate equality in procedural knowledge.

    Instead of the Sisyphean task of forever translating these ideas from scratch, coders need to step back and think: Is it possible to just encode the axioms and enough knowledge to derive a family of algorithms, then let the compiler derive the most appropriate procedure for computing some desired quantity? A good compiler might be able to use those facts to optimize computation graphs, e.g. for latency or numerical stability. This has been the holy grail of declarative programming.

    Short of that, can we have some kind of \textit{bibliotheca universalis} containing many human-written code examples, which a human could select from manually (a la Hoogle~\citep{james2020digging}) -- or even better -- which could be linked to during compilation. We can think of big code as a kind of procedural knowledge system, describing common data transformations. We would like a way to extract common snippets and reason about those transformations, for example to detect similar procedures or optimize an existing procedure.

    Knowledge systems or \textit{ontologies} are a collection of related facts which have been established by human beings. For example, we can treat mathematics as a knowledge base of rewrite rules. This has been successfully operationalized in Theano~\citep{bergstra2010theano}, Kotlin$\nabla$~\citep{considine2019kotlingrad} and other DSLs. More broadly, we can also think of constructive mathematics libraries like Metamath~\citep{megill2006metamath}, Rubi~\citep{rich2009knowledge}, Probonto~\citep{swat2016probonto} and KMath~\citep{nozik2019kotlin} as working towards this same goal.

    In knowledge graphs, approximate equality is known as entity \textit{alignment} or \textit{matching}. With a probabilistic matching algorithm, we could accurately detect near duplicates in a codebase. We could retrieve code samples to assist developers writing unfamiliar code. And we could search for bugs in code or fixes from a knowledge base to repair them. Probabilistic reasoning can be gainfully employed on these and many related tasks.

    Suppose we want to search through a software knowledge base for an error and stack trace, then use the information in the KB to repair our bug.

    \begin{enumerate}
        \item Efficiently searching corpus for a pattern
        \item Identifying alignment and matching results
        \item Incorporating information into user's context
    \end{enumerate}

    \subsection{Code search}

    model fragments of code and natural language as a graphs and learn a distance metric which captures the notion of similarity. Some graphs will be incomplete, or missing some features, others will have extra information that is unnecessary.

    Given a piece of code and the surrounding context (e.g. in an IDE or compiler), search a database for the most similar graphs, then to recommend them to the user (e.g. fixes or repairs for compiler error messages), or suggest some relevant examples to help the user write some incomplete piece of code.It is similar to a string edit distance, but for graph structured objects. There are a few pieces to this:

    \begin{enumerate}
        \item Semantic segmentation (what granularity to slice?)
        \item Graph matching (how to measure similarity?)
        \item Graph search (how to search efficiently?)
        \item Recommendation (how to integrate into user's code)
    \end{enumerate}

    The rewriting mechanism is similar to a string edit distance, but for graphs. One way of measuring distance could be measuring the shortest number of steps for rewriting a graph A to graph B, i.e. the more "similar" these two graphs are the fewer rewriting steps it should take.

    \subsection{Fault localization}

    searching for stack trace on stackoverflow

    \subsection{Program repair}

    adapt some knowledge to match user's context

    \subsection{eDSL generation}

    Take a procedural knowledge base, generate a DSL from it.

    \pagebreak
    \bibliography{exam_proposal}
    \bibliographystyle{plain}
\end{document}
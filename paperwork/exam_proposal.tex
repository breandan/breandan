%! Author = breandan
%! Date = 11/16/20

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{prooftrees}
\usepackage{natbib}

\usepackage{xcolor}
\pagecolor[rgb]{0.1,0.1,0.1} %black
\color[rgb]{0.7,0.7,0.7} %grey



\title{Comprehensive Exam Syllabus}
\author{Breandan Considine}
\date{\today}

% Document
\begin{document}
    \maketitle

    \tableofcontents

    TODO: think of a good title

    \begin{enumerate}
      \item Pattern recognition in structured documents?
      \item Towards a notion of sameness in software artifacts
      \item Computer-Aided Programming...
      \item Knowledge-driven development
      \item Content recommendation for programming tools
      \item Reasoning and relating software artifacts
      \item Traceable documentation retrieval
      \item Entity matching for programming recommendations
    \end{enumerate}

    Prior work~\citep{considine2019kotlingrad,considine2019programming} explored differentiation. The notion of differentiation is very important for optimization. But ultimately it gives us an incomplete picture. In this work, we attempt to establish the connection between probabilistic reasoning and relational reasoning using antidifferentiation.

    The notion of equivalence has been studied across many logical disciplines. Broadly speaking, there are various properties we can use to determine whether two objects are equal. We can probe at their features, rewrite them, etc.

    But why do we care about equivalence? By identifying equivalent objects, we can find objects in a database to increase code sharing and reduce duplication. We can also reason about functions and relations between those objects.

    This has many applications in knowledge systems. Identifying when two entities are the same is called entity matching or alignment. We can identify and discover related concepts.

    Probabilistically, we care about distribution matching and distance metrics (e.g. earthmover, Kantorovich-Rubinstein, Prokhorov et al.)


    Basically, we want to reason about probabilistic objects. Natural languages are imprecise. Unclear semantics. Can we still do inference?

    This is all related to probabilistic [logic] programming. We want to ask MAR, MAP, EVI queries. We need a formal language to describe probabilistic relations on natural languages.


    In the following sections, we will define exact and approximate equality in the discrete and continuous domain, how to decide it exactly and approximately, then describe a few applications for software engineering.

    \section{Equality, Formally}

    In mathematical equality, two expressions are considered equal if, loosely speaking, they represent the same concept. The problem is, mathematical equality is not differentiable.

    It has the following logical properties:

    \begin{enumerate}
        \item $\implies a = a$
        \item $a = b \implies b = a$
        \item $a = b$ and $b = c \implies a = c$
        \item $\forall F, a = b \implies F(a) = F(b)$
    \end{enumerate}

    Consider the Kronecker delta function $\delta_k: \mathcal{T}$, where $\mathcal{T} \in \mathbb{Z, Q, B}$:

    $$
    \delta_k(i, j) :=
    \begin{cases}
        1 \text{ if } i = j, \\
        0 \text{ otherwise }\\
    \end{cases}
    $$

    However it is not a continuous function: the preimage of an open set is not open. Thus, it is not differentiable.

    Now consider the Dirac delta function, $\delta_d$. Although it cannot be written down directly, it can be described indirectly:

    $$
    \delta_d(i, j) :=
    \begin{cases}
        \infty \text{ if } i = j, \\
        0 \text{ otherwise }\\
    \end{cases}

    \text{ where } \int \delta_d (x)dx = 1
    $$

    What is the difference between mathematical functions and software? Software implements $\delta_k$, while mathematical functions implement $\delta_d$.

    Both of these are idealized concepts which do not exist in the real world. Unknowable whether the universe is discrete or continuous. All we can do is approximate and compare empirically.

    However the fact that many humans have converged on these definitions, captures an aspect of the human brain that we would expect a synthetic reasoner to also possess.

    To allow a more realistic version the $=$ operator, we require a notion of equality which approximates the logical properties of $\delta_k$, but which can be made differentiable like $\delta_d$.

    \subsection{Incompleteness}

    Mathematics is often a useful approximation of reality, but the trouble is, many mathematical concepts are impossible to realize in the real world.

    This is due to two problems:

    \begin{enumerate}
        \item intrinsic: the mechanics of the system we have written down are fundamentally unsound and do not admit mechanization.
        \item extrinsic: all descriptions (epistemic) and observations (alleoteric) are imprecise and this imprecision can compound quickly.
    \end{enumerate}

    Even very simple formulas containing arithmetic are undecidable. (Peano)

    No complete and consistent formal systems: we cannot construct a machine implementing a program of mathematics. (Godel)

    In general, determining equality between two mathematical formulas is undecidable, even very simple formulas containing arithmetic. (Richardson)

    More generally, any nontrivial property is undecidable. (Rice)

    But can we organize mathematical knowledge as a rule based decision tree? (Rich)

    Mathematics seems to be obsessed with completeness, much to its detriment. Completeness invades every definition. Definitions must hold over all cases, including the adversarial ones. This is good, because it encourages rigor in a highly adversarial field.

    But most people prefer useful abstractions to complete specifications. Most people just want to write dependable software.

    To be more inclusive of our audience, we should optimize our definitions for consistency, at the expense of completeness.

    In fact, we can answer many questions exactly. If we restrict the reasoning system to incomplete queries, we can be consistient. In Section~\ref{sec:structural}, we will show some nice algorithms for doing so.

    \subsection{Intensional equivalence}\label{subsec:intensional-equivalence}

    Let $R: (\mathcal{I} \rightarrow \mathcal{O}) \times (\mathcal{I}\rightarrow \mathcal{O})$ be a relation between functions which is closed under composition. We say that $f_1$ and $f_2$ are intensionally equal if $R_n(f_1)=R_m(f_2)$ for some $m,n \in \mathbb{Z}$. For example, suppose we have two functions $f_1, f_2: \mathbb{Z}^3 \rightarrow \mathbb{Z}$ where $f_1(x, y, z)=xz + xy$ and $f_2(x, y, z)=x(y + z)$, and $\mathcal{R}={a + b := b + a, a(b + c) := ab + ac}$ is our rewrite system. If we apply $R$ twice to $f_2$, we obtain $R_2(f_2)=x(y + z):=xy + xz:=xz + xy=R_0(f_1)$ and thus $f_1$ and $f_2$ are both intensionally equal.

    \subsection{Extensional equivalence}\label{subsec:extensional-equivalence}

    We say $f_1$ and $f_2$ are extensionally equivalent if $\forall i \in \mathcal{I}, f_1(i)=f_2(i)$, or in other words, "Do $f_1$ and $f_2$ behave in the same way over all inputs?" While extensional equality is difficult to show if $|\mathcal{I}|$ is large, detecting inequality for pairs of random functions is relatively easy: we can simply search for $\hat i \in \mathcal{I}$ such that $f_1(\hat i) \neq f_2(\hat i)$. For example, suppose we have two functions $f_1, f_2: \mathbb{Z}^3 \rightarrow \mathbb{Z}$ where $f_1(x)=-x^3$ and $f_2(x)=|x|^3$. If we test $\hat i \in {-2, -1, 0, 1, \ldots}$, we have $f_1(-2)=f_2(-2)$, $f_1(-1)=f_2(-1)$, $f_1(0)=f_2(0)$, $f_1(1) \neq f_2(1)$. Once we detect an $f_1(\hat i) \neq f_2(\hat i)$, we can halt immediately.

    \subsection{Observational Equivalence}

    Two terms $\textt{M}$ and $\textt{N}$ are observationally equivalent iff $\forall \textt{C[...]}$ where $\textt{C[M]}$ is valid and halts, $\textt{C[N]}$ is also valid and halts.

    This is kind of like word embeddings.

    $P(w_t = a | w_{t-1}, w_{t-2}\ldots, w_{t+1}, w_{t+2}, \ldots)\overset{?}{\approx} P(w_t = b | w_{t-1}, w_{t-2}\ldots, w_{t+1}, w_{t+2}, \ldots)$

    We can define semantic equality in our setting using word embeddings.

    \section{Structural Equality}\label{sec:structural}

    So far, we have described the properties of equality, but we still need an algorithm for how to establish it. We need a decision algorithm.

    It is seldom the case that two equal things are trivially equal. We need to do some computation to put them into a form that makes their equality plain to see.

    To establish equality, we need some notion of a "transformation", or "rewrite".

    Equivalence requires:

    1. reducing an object into a normal form (canonicalization),

    2. doing some kind of comparison. At this point, equality should be trivial (e.g. using string comparison)


    Knuth-Bendix completion is a semialgorithm...

    Suppose we have two functions $f_1: \mathcal{I} \rightarrow \mathcal{O}$ and $f_2: \mathcal{I}\rightarrow \mathcal{O}$.


    \subsection{Isomorphism}

    [Sub]graph isomorphism


    \subsection{Metamorphism}

    Metamorphic testing

    \section{Approximate Equality}

    We want a metric $M_\theta: (\mathcal{I}\rightarrow{O}) \times (\mathcal{I}\rightarrow{O})\rightarrow \mathbb{R}$ between two functions, which predicts their semantic similarity. In other words, the closer two functions are with respect to $M_\theta$, the more likely they are to be equal. During test time, we query a dataset for the $k$ most similar functions, and try to unify them extensionally and intensionally. We can train a metric $M_\theta$ and discriminator $D_\theta$ on pairs of random functions $f_1$ and $f_2$, to predict their similarity. During inference, we let the discriminator sample random inputs $\hat i_1 \ldots n \sim D_\theta(\hat i \mid f_1, f_2)$ from its latent distribution, conditioning on the structure of $f_1$ and $f_2$. In other words, we want a model that predicts similarity and outputs values which are likely to demonstrate instances of inequality.

    \section{Software equality}

    We want to search through a software knowledge base for an error and stack trace, then use the information in the KB to repair our bug.

    \begin{enumerate}
        \item Efficiently searching corpus for a pattern
        \item Identifying alignment and matching results
        \item Incorporating information into user's context
    \end{enumerate}


    \subsection{Code clone detection}

    \subsection{Knowledge alignment}

    Knowledge alignment, entity matching, ...

    \subsection{Graph rewriting}

    \subsection{Graph edit distance}

    model fragments of code and natural language as a graphs and learn a distance metric which captures the notion of similarity. Some graphs will be incomplete, or missing some features, others will have extra information that is unnecessary.

    Given a piece of code and the surrounding context (e.g. in an IDE or compiler), search a database for the most similar graphs, then to recommend them to the user (e.g. fixes or repairs for compiler error messages), or suggest some relevant examples to help the user write some incomplete piece of code.It is similar to a string edit distance, but for graph structured objects. There are a few pieces to this:

    \begin{enumerate}
    \item Semantic segmentation (what granularity to slice?)
    \item Graph matching (how to measure similarity?)
    \item Graph search (how to search efficiently?)
    \item Recommendation (how to integrate into user's code)
    \end{enumerate}

    The rewriting mechanism is similar to a string edit distance, but for graphs. One way of measuring distance could be measuring the shortest number of steps for rewriting a graph A to graph B, ie. the more "similar" these two graphs are the fewer rewriting steps it should take.

    \section{Future work}

    Semirings arise in strange and marvelous places. (min, +), (max, \times)

    \begin{enumerate}
    \item https://people.cs.kuleuven.be/~luc.deraedt/Francqui4ab.pdf#page=71
    \item http://www.mit.edu/~kepner/GraphBLAS/GraphBLAS-Math-release.pdf#page=11
    \end{enumerate}

    \bibliography{exam_proposal}
    \bibliographystyle{plain}
\end{document}
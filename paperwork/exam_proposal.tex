%! Author = breandan
%! Date = 11/16/20

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{prooftrees}
\usepackage{bussproofs}
\usepackage{hyperref}
\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}
\usepackage{natbib}
\usepackage{float}
\usepackage{xcolor}


\title{Comprehensive Exam Syllabus}
\author{Breandan Considine}
\date{\today}

% Document
\begin{document}
    \maketitle

    \tableofcontents
    \pagebreak

%    TODO: think of a good title
%
%    \begin{enumerate}
%      \item Pattern recognition in structured documents?
%      \item Towards a notion of sameness in software artifacts
%      \item Computer-Aided Programming...
%      \item Knowledge-driven development
%      \item Content recommendation for programming tools
%      \item Reasoning and relating software artifacts
%      \item Traceable documentation retrieval
%      \item Entity matching for programming recommendations
%    \end{enumerate}

    \section{Introduction}

    Historically, most knowledge was recorded as natural language. A growing portion is now \textit{code}~\citep{allamanis2018survey}. Code is procedural knowledge intended for execution by a machine. Though it shares many statistical properties in common with natural language~\citep{hindle2012naturalness}, code is written in a formal language with a deterministic grammar and denotational semantics~\citep{pierce2010software}. We can use this specification to precisely reason about operational or procedural correctness.

    Prior work~\citep{considine2019kotlingrad,considine2019programming} explored differentiable programming. Differentiability plays a key role in learning, but does not provide the vocabulary necessary to describe human knowledge. In order to capture human knowledge and begin to reason as humans do, programs must be able to express the notion of \textit{uncertainty}. In this work, we propose a set of tools and techniques for reasoning about uncertainty in the form of procedural knowledge.

    To reason about procedural knowledge, we must first define what it means for two procedures to be equal. This has been studied in many logical disciplines, which have proposed various algorithms for determining equality according to some language or axioms. For example, we might substitute various instructions~\citep{baader1999term}, run them on various inputs~\citep{chen2020metamorphic}, or compare them in various contexts~\citep{felleisen1990expressive} so as to ascertain their exact relationship.

    In practice, exact equality is too rigid to operationalize. A more useful theory would allow us to compare two procedures in the presence of naturally-arising stochasticity. What is the probability of observing local variations? How are those observations related? And how do local variations affect global properties? In order to correctly pose these questions and begin to answer them, we must be able to reason probabilistically~\citep{pearl2014probabilistic}.

    In knowledge graphs, approximate equality is known as entity \textit{alignment} or \textit{matching}. With a probabilistic matching algorithm, we could accurately detect near duplicates in a codebase. We could retrieve code samples to assist developers writing unfamiliar code. And we could search for bugs in code or fixes from a knowledge base to repair them. Probabilistic reasoning can be gainfully employed on these and many related tasks.

    In the following document, we will first define exact and approximate equality in the deterministic and probabilistic setting (~\ref{sec:definitions}). We will then describe some strategies for deciding equality using rewriting and equivalence testing (~\ref{sec:algorithms}). Finally, we will discuss a few opportunities for applying probabilistic reasoning to search-based software engineering, in particular duplicate detection, fault localization and program repair (~\ref{sec:applications}).

    \section{Defining Equality}\label{sec:definitions}

    Equality is a highly overloaded notation in mathematics and computer science. Roughly, $i = j$ denotes when we can replace, or are replacing an expression $i$ with an expression $j$. Two expressions are considered equal if, loosely speaking, they evaluate to the same result.

    The problem is, equality is not a differentiable operator. If two expressions are unequal, the derivative alone cannot tell us how to fix them. When implementing a differentiable program using the expression $\texttt{if(i == j)}$, we are forced to use one of two definitions, neither of which are satisfactory. Consider the Kronecker delta function $\delta_k: \mathbb{T}$, where $\mathbb{T} \in \{\mathbb{Z, Q, B}\}$:

    $$
    \delta_k(i, j) :=
    \begin{cases}
        1 \text{ if } i = j, \\
        0 \text{ otherwise }\\
    \end{cases}
    $$

    This is not a continuous function: the preimage of an open set is not open. Thus, it is not differentiable. Maybe a way to fix it, e.g. CADLAG. Now consider the Dirac delta function, $\delta_d: \mathbb{R} \rightarrow \mathbb{R}$. Although it cannot be written down directly, it can be described indirectly:

    $$
    \int_{-\infty}^{\infty} \delta_d (x)dx = 1 \text{ and } \delta_d(i, j) :=
    \begin{cases}
        \infty \text{ if } i = j, \\
        0 \text{ otherwise }\\
    \end{cases}
    $$

    Neither of these definitions is particularly appealing. Do we really need $\infty$ just to describe equality? Without knowing $i$ or $j$ a priori, how do we optimize such a function using gradient based techniques?

%    Both of these are idealized concepts which do not exist in the real world. Unknowable whether the universe is discrete or continuous. Most software is discrete. All we can do is approximate and compare empirically.

    A more general notion is the notion of an \textit{equivalence relation}. An equivalence relation $\sim$ is a binary relation with the following logical properties:

    \begin{prooftree}
        \AxiomC{}
        \UnaryInfC{$a \sim a$}
        \DisplayProof
        \hskip 1.5em
        \AxiomC{$a \sim b$}
        \UnaryInfC{$b \sim a$}
        \DisplayProof
        \hskip 1.5em
        \AxiomC{$a \sim b$}
        \AxiomC{$b \sim c$}
        \BinaryInfC{$a \sim c$}
        \DisplayProof
        \hskip 1.5em
        \AxiomC{$a \sim b$}
        \AxiomC{$F: A \rightarrow *$}
        \BinaryInfC{$F(a) \sim F(b)$}
    \end{prooftree}

    The fact that many humans have converged to these definitions captures an aspect of the human brain that we would expect a synthetic reasoner to also possess. To allow a more flexible version of the $=$ operator, we require a probability distribution: a notion of equality which approximates the logical properties of $\delta_k$, but which can be made differentiable like $\delta_d$.

    \subsection{Intensional equivalence}\label{subsec:intensional-equivalence}

    Let $R: (\mathcal{I} \rightarrow \mathcal{O}) \times (\mathcal{I}\rightarrow \mathcal{O})$ be a relation between functions which is closed under composition. We say that $f_1$ and $f_2$ are intensionally equal if $R_n(f_1)=R_m(f_2)$ for some $m,n \in \mathbb{Z}$. For example, suppose we have two functions $f_1, f_2: \mathbb{Z}^3 \rightarrow \mathbb{Z}$ where $f_1(x, y, z)=xz + xy$ and $f_2(x, y, z)=x(y + z)$, and $\mathcal{R}={a + b := b + a, a(b + c) := ab + ac}$ is our rewrite system. If we apply $R$ twice to $f_2$, we obtain $R_2(f_2)=x(y + z):=xy + xz:=xz + xy=R_0(f_1)$ and thus $f_1$ and $f_2$ are both intensionally equal.

    \subsection{Extensional equivalence}\label{subsec:extensional-equivalence}

    We say $f_1$ and $f_2$ are extensionally equivalent if $\forall i \in \mathcal{I}, f_1(i)=f_2(i)$, or in other words, ``Do $f_1$ and $f_2$ behave in the same way over all inputs?" While extensional equality is difficult to show if $|\mathcal{I}|$ is large, detecting inequality for pairs of random functions is relatively easy: we can simply search for $\hat i \in \mathcal{I}$ such that $f_1(\hat i) \neq f_2(\hat i)$. For example, suppose we have two functions $f_1, f_2: \mathbb{Z}^3 \rightarrow \mathbb{Z}$ where $f_1(x)=-x^3$ and $f_2(x)=|x|^3$. If we test $\hat i \in {-2, -1, 0, 1, \ldots}$, we have $f_1(-2)=f_2(-2)$, $f_1(-1)=f_2(-1)$, $f_1(0)=f_2(0)$, $f_1(1) \neq f_2(1)$. Once we detect an $f_1(\hat i) \neq f_2(\hat i)$, we can halt immediately.

    \subsection{Observational equivalence}

    Two terms $\textt{M}$ and $\textt{N}$ are observationally equivalent iff $\forall \textt{C[...]}$ where $\textt{C[M]}$ is valid in our language and halts, $\textt{C[N]}$ is also valid and halts. This is kind of like word embeddings.

    $P(w_t = a | w_{t-1}, w_{t-2}\ldots, w_{t+1}, w_{t+2}, \ldots)\overset{?}{\approx} P(w_t = b | w_{t-1}, w_{t-2}\ldots, w_{t+1}, w_{t+2}, \ldots)$

    We can define semantic equality in our setting using word embeddings.

    \section{Deciding Equality}\label{sec:algorithms}

    Various methods have been proposed to decide exact and approximate equality in the deterministic and probabilistic setting.

    {\centering
    \begin{table}[H]
        \begin{tabular}{c|c|c|}
            \cline{2-3} & \textbf{Deterministic} & \textbf{Probabilistic} \\ \hline
            \multicolumn{1}{|l|}{\textbf{Exact}}       & \begin{tabular}[c]{@{}l@{}}Types\\ Rewriting\end{tabular}   & \begin{tabular}[c]{@{}l@{}}Probabilistic circuits\\ Variable elimination\end{tabular} \\ \hline
            \multicolumn{1}{|l|}{\textbf{Approximate}} & \begin{tabular}[c]{@{}l@{}}Testing\\ Debugging\end{tabular} & \begin{tabular}[c]{@{}l@{}}Sampling\\ Simulation\end{tabular}                         \\ \hline
        \end{tabular}
    \end{table}
    }

    So far, we have described the properties of equality, but we still need an algorithm for how to establish it. We need a decision algorithm.

    It is seldom the case that two equal things are trivially equal. We need to do some computation to put them into a form that makes their equality plain to see.

    For exact equality, we need some notion of a transformation or rewrite. Equivalence requires:

    1. reducing an object into a normal form (canonicalization), then

    2. doing some kind of trivial comparison (e.g. string comparison)

    For approximate equality, we can just generate some representative data and test away.

    \subsection{Equality is not decidable}

    Mathematics is often a useful approximation of reality, but many mathematical concepts are impossible to realize. This is due to two problems:

    \begin{enumerate}
        \item intrinsic: the mechanics of the system are intrinsically unsound or impossible to mechanize. How do we encode $\lim_{x \rightarrow \infty}$?
        \item extrinsic: all descriptions (epistemic) and observations (alleoteric) are approximate and this error can compound quickly.
    \end{enumerate}

    Even very simple formulas containing arithmetic are undecidable. (Peano)

    No complete and consistent formal systems: we cannot construct a machine implementing the program of mathematics. (Godel)

    Determining equality between two mathematical formulas is undecidable. (Richardson)

    More generally, any nontrivial property is undecidable. (Rice)

    But there is hope! (Felleisen)

%    But can we organize mathematical knowledge as a decision tree? (Rich)

    Still, we can answer many questions exactly. If we restrict the reasoning system to incomplete queries, we can be consistient. In Section~\ref{sec:structural}, we will show some nice algorithms for doing so.

%    Mathematics seems to be obsessed with completeness, much to its detriment. Completeness invades every definition. Definitions must hold over all cases, including the adversarial ones. This is good, because it encourages rigor in a highly adversarial field. But we must not confuse rigor with truth.
%
%    The truth is, most people prefer useful abstractions to complete specifications. Most people just want to write dependable software.
%
%    To be more inclusive of our audience, we should optimize our definitions for consistency, at the expense of completeness.

    \subsection{Axiomatic approaches}

    We can rewrite stuff using rewrite systems.

    Knuth-Bendix completion is a semialgorithm...

    Suppose we have two functions $f_1: \mathcal{I} \rightarrow \mathcal{O}$ and $f_2: \mathcal{I}\rightarrow \mathcal{O}$.


    \subsection{Software approaches}

    We can test using property-based / metamorphic testing. Some strategies:

    Isomorphism testing
    Metamorphic testing

    \subsection{Probabilistic approaches}

    How do we know when we are approaching equality? We can use a distance metric.

    Kolmogorov-Smirnov

    Kantorovich-Rubinstein

    L\'evy-Prokhorov

    TODO: Define probability distributions, integration, kernel functions and metrics.

    We want a metric $M_\theta: (\mathcal{I}\rightarrow{O}) \times (\mathcal{I}\rightarrow{O})\rightarrow \mathbb{R}$ between two functions, which predicts their semantic similarity. In other words, the closer two functions are with respect to $M_\theta$, the more likely they are to be equal. During test time, we query a dataset for the $k$ most similar functions, and try to unify them extensionally and intensionally. We can train a metric $M_\theta$ and discriminator $D_\theta$ on pairs of random functions $f_1$ and $f_2$, to predict their similarity. During inference, we let the discriminator sample random inputs $\hat i_1 \ldots n \sim D_\theta(\hat i \mid f_1, f_2)$ from its latent distribution, conditioning on the structure of $f_1$ and $f_2$. In other words, we want a model that predicts similarity and outputs values which are likely to demonstrate instances of inequality.

    \section{Developing Software}\label{sec:applications}

    We want to search through a software knowledge base for an error and stack trace, then use the information in the KB to repair our bug.

    \begin{enumerate}
        \item Efficiently searching corpus for a pattern
        \item Identifying alignment and matching results
        \item Incorporating information into user's context
    \end{enumerate}


    \subsection{Code search}

    We can search for code using various strategies...

    \subsection{Knowledge alignment}

    Knowledge alignment, entity matching, ...

    \subsection{Graph rewriting}

    \subsection{Graph edit distance}

    model fragments of code and natural language as a graphs and learn a distance metric which captures the notion of similarity. Some graphs will be incomplete, or missing some features, others will have extra information that is unnecessary.

    Given a piece of code and the surrounding context (e.g. in an IDE or compiler), search a database for the most similar graphs, then to recommend them to the user (e.g. fixes or repairs for compiler error messages), or suggest some relevant examples to help the user write some incomplete piece of code.It is similar to a string edit distance, but for graph structured objects. There are a few pieces to this:

    \begin{enumerate}
    \item Semantic segmentation (what granularity to slice?)
    \item Graph matching (how to measure similarity?)
    \item Graph search (how to search efficiently?)
    \item Recommendation (how to integrate into user's code)
    \end{enumerate}

    The rewriting mechanism is similar to a string edit distance, but for graphs. One way of measuring distance could be measuring the shortest number of steps for rewriting a graph A to graph B, ie. the more "similar" these two graphs are the fewer rewriting steps it should take.

    \subsection{Future work}

    Semirings arise in strange and marvelous places. (min, +), (max, \times)

    \begin{enumerate}
    \item https://people.cs.kuleuven.be/~luc.deraedt/Francqui4ab.pdf#page=71
    \item http://www.mit.edu/~kepner/GraphBLAS/GraphBLAS-Math-release.pdf#page=11
    \end{enumerate}

    \bibliography{exam_proposal}
    \bibliographystyle{plain}
\end{document}